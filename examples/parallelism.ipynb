{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "443a50ff-d105-4a10-9363-050725fe21df",
   "metadata": {},
   "source": [
    "# Autoparallelism (running on multiple GPUs)\n",
    "\n",
    "It's very common to have a machine with multiple GPUs, and to seek to parallelise your computation over them.\n",
    "\n",
    "JAX has a number of advanced APIs to support this. The main technique is to \"shard\" an array, so that each device holds part of the array.\n",
    "\n",
    "In this example, we'll parallelise our computation (usually it's a training step) over 8 devices, so that each device gets 1/8 of the batch of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a0bae8-2435-4b37-b1f2-24322cfeb1dd",
   "metadata": {},
   "source": [
    "First let's import everything, and set up our toy problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83bba892-5425-4eed-a7f7-9c325fe5cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.experimental.mesh_utils as mesh_utils\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.sharding as sharding\n",
    "import numpy as np\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "\n",
    "# Hyperparameters\n",
    "dataset_size = 64\n",
    "channel_size = 4\n",
    "hidden_size = 32\n",
    "depth = 1\n",
    "learning_rate = 3e-4\n",
    "num_steps = 10\n",
    "batch_size = 16  # must be a multiple of our number of devices.\n",
    "\n",
    "# Generate some synthetic data\n",
    "xs = np.random.normal(size=(dataset_size, channel_size))\n",
    "ys = np.sin(xs)\n",
    "\n",
    "model = eqx.nn.MLP(channel_size, channel_size, hidden_size, depth, key=jr.PRNGKey(6789))\n",
    "optim = optax.adam(learning_rate)\n",
    "opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "\n",
    "\n",
    "def compute_loss(model, x, y):\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    return jnp.mean((y - pred_y) ** 2)\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(model, opt_state, x, y):\n",
    "    grads = eqx.filter_grad(compute_loss)(model, x, y)\n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb345b0-c9b3-44df-94e8-d74c7ad172b8",
   "metadata": {},
   "source": [
    "Here's a very simple dataloader, that randomly shuffles and slices our dataset. We keep everything in pure-NumPy for speed, as this all happens on the host, prior to moving our data to our devices. (Which will often be a cluster of GPUs.)\n",
    "\n",
    "In practice it's also common to load data using either PyTorch's `DataLoader` or TensorFlow's `tf.data` API; see [here](../mnist/) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd94db04-9fe4-4530-808e-945becef9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = np.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = np.random.permutation(indices)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end <= dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da180e54-0deb-4e33-be7b-9856049cd483",
   "metadata": {},
   "source": [
    "Now the magic: create our sharding object, move our data on to our devices, and run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32c6b58e-f72f-4dd4-bf2c-f1dc75643eda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_devices = len(jax.devices())\n",
    "devices = mesh_utils.create_device_mesh((num_devices, 1))\n",
    "shard = sharding.PositionalSharding(devices)\n",
    "\n",
    "for step, (x, y) in zip(range(num_steps), dataloader((xs, ys), batch_size)):\n",
    "    x, y = jax.device_put((x, y), shard)\n",
    "    model, opt_state = make_step(model, opt_state, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee431d2-41f4-4e96-89f2-47e239d92574",
   "metadata": {},
   "source": [
    "**That's it!**\n",
    "\n",
    "Once you've specified how you want to split up your input data, then JAX does the rest of it for you! It takes your single JIT'd computation (which you wrote as if you were targeting a single huge device), and it then automatically determined how to split up that computation and have each device handle part of the computation. This is JAX's computation-follows-data approach to autoparallelism.\n",
    "\n",
    "If you ran the above example on a cluster of NVIDIA GPUs, then you can check whether you're using as many GPUs as you expected by running `nvidia-smi` from the command line. You can also use `jax.debug.visualize_array_sharding(array)` to inspect the sharding manually.\n",
    "\n",
    "One possible optimisation here is to re-use the memory used by the input arrays, to store the output arrays. This often improves speed a little bit. This is disabled by default, but can be enabled by passing `eqx.filter_jit(donate=\"all\")`.\n",
    "\n",
    "**What about pmap?**\n",
    "\n",
    "The JAX team have been hard at work introducing these new easy-to-use parallelism features, based around JIT and sharding. These are often faster and more expressive than pmap, so pmap is no longer recommended!\n",
    "\n",
    "**Types of parallelism**\n",
    "\n",
    "There are multiple types of parallelism. In this example we demonstrated _data parallelism_, in which we parallelise over the data. This is one of the simplest to set up, and often very effective.\n",
    "\n",
    "For completeness we note that there are other kinds of parallelism available -- e.g. model parallelism, which instead places different parts of the model on different devices. A discussion on those is a more advanced topic. :)\n",
    "\n",
    "**Further reading**\n",
    "\n",
    "Equinox works smoothly with all the built-in parallelism APIs provided by JAX. If you want to know more, then the relevant parts of the JAX documentation are:\n",
    "\n",
    "- [Distributed arrays and automatic parallelization](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html): a first introduction to JAX's autoparallelism APIs.\n",
    "- [Using JAX in multi-host and multi-process environments](https://jax.readthedocs.io/en/latest/multi_process.html): an advanced discussion on running JAX with multiple hosts. (In this example we had a single \"host\" -- the driving Python script -- and multiple \"devices\" -- the GPUs we were farming work out to.)\n",
    "- [shmap (shard_map) for simple per-device code](https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html): an experimental new API for explicitly controlling how a computation should be split across devices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax39",
   "language": "python",
   "name": "jax39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
