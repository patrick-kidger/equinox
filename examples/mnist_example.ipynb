{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4d995728-cc8a-470b-a787-00152881c3c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all the necessary libraries\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torchvision\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "20a8b086-171d-48c1-acd0-2ad35641000b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "40aaceb7-118a-448e-9e6a-1df2b51eec55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(label):\n",
    "    \"\"\"\n",
    "        This function one-hot encodes the target label.\n",
    "        E.g.:\n",
    "            label = 3\n",
    "            output = [0 0 0 1 0 0 0 0 0 0]\n",
    "    \"\"\"\n",
    "    one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "    one_hot_encoder.fit(np.arange(10).reshape(-1, 1))\n",
    "    return one_hot_encoder.transform(np.array(label).reshape(1, 1)).reshape(-1)\n",
    "\n",
    "# transforms\n",
    "# This normalises the data and then flattens the 28x28 image to a 784 dimensional vector\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)), transforms.Lambda(lambda x: torch.flatten(x))]\n",
    ")\n",
    "\n",
    "\n",
    "# Load the MNIST train data\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=one_hot_encode,\n",
    ")\n",
    "\n",
    "# Load the MNIST test data\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=one_hot_encode,\n",
    ")\n",
    "\n",
    "# Create the train and testloaders\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fd42ccf7-b192-41a2-b41f-991feeb4d6fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(eqx.Module):\n",
    "    layers: list\n",
    "    \n",
    "    def __init__(self, key):\n",
    "        key1, key2, key3 = jax.random.split(key, 3)\n",
    "        # A simple neural network with 3 hidden layers\n",
    "        self.layers = [\n",
    "            eqx.nn.Linear(784, 64, key=key1),\n",
    "            eqx.nn.Linear(64, 32, key=key2),\n",
    "            eqx.nn.Linear(32, 10, key=key3)\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            # call relu on each layer\n",
    "            x = jax.nn.relu(layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6679259e-c88a-4711-85cc-877abe6a94aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss(model, x, y):\n",
    "    # Our input has the shape (BATCH_SIZE x 784) but our model expects an input of (784, ) \n",
    "    # Therefore, we have to use jax.vmap, which - by default - maps over the 0th axis (i.e. the batch in our example)\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    # mean squared error loss\n",
    "    return jnp.mean((y - pred_y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b0d4ae5d-ed7d-4e49-9bfe-4345da704be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Init our model\n",
    "key, subkey = jax.random.split(jax.random.PRNGKey(32))\n",
    "model = NeuralNetwork(subkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6bf77daf-9a95-4428-ab55-b43e344353dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 784)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking our data a bit (by now, everyone knows what the MNIST dataset looks like)\n",
    "dummy_x, dummy_y = next(iter(trainloader))\n",
    "dummy_x = dummy_x.numpy()\n",
    "dummy_y = dummy_y.numpy()\n",
    "\n",
    "dummy_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2744d36a-0201-40ed-abe5-afa3f0949870",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 10)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example inference\n",
    "output = jax.vmap(model)(dummy_x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "92035784-0ce5-4dc6-968f-d6637915c907",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09782601\n"
     ]
    }
   ],
   "source": [
    "# Getting the initial loss value\n",
    "value, grads = jax.value_and_grad(loss)(model, dummy_x, dummy_y)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b8ef2daa-a5b8-4963-8cda-7b80f736880d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, testloader: DataLoader):\n",
    "    \"\"\"\n",
    "        This function evaluates the model on the testing dataset by computing the average loss and also the average accuracy.\n",
    "    \"\"\"\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    for x, y in testloader:\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        avg_loss += loss(model, x, y)\n",
    "        preds = jax.vmap(model)(x)\n",
    "        targets = jnp.argmax(y, axis=1)\n",
    "        preds = jnp.argmax(preds, axis=1)\n",
    "        avg_acc += sum(targets == preds) / len(x)\n",
    "    return avg_loss / len(testloader), avg_acc / len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e8d453ad-da34-4c48-8b8e-d94574144aae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(0.0979604, dtype=float32),\n",
       " Array(0.11445063, dtype=float32, weak_type=True))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2861f671-6a68-4f27-9e4c-987d893df076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use Optax to optimise the parameters\n",
    "optim = optax.adamw(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "187980de-563e-4d1c-826f-572a4157a29a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(model, trainloader: DataLoader, testloader: DataLoader, optim):\n",
    "    # Get the optimiser state\n",
    "    opt_state = optim.init(model)\n",
    "    \n",
    "    @jax.jit\n",
    "    def step(model, opt_state, x, y):\n",
    "        loss_value, grads = jax.value_and_grad(loss)(model, x, y)\n",
    "        updates, opt_state = optim.update(grads, opt_state, model)\n",
    "        model = optax.apply_updates(model, updates)\n",
    "        return model, opt_state, loss_value\n",
    "    \n",
    "    for i, (x, y) in enumerate(trainloader):\n",
    "        # PyTorch dataloaders give torch tensors by default, so we have to convert them to Numpy arrays first\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        model, opt_state, loss_value = step(model, opt_state, x, y)\n",
    "        # Every 10% of the dataset, print a snapshot of the model's performance\n",
    "        if i % (len(trainloader) // 10) == 0:\n",
    "            eval_loss, acc = evaluate(model, testloader)\n",
    "            print(f\"step={jnp.round((i / len(trainloader)) * 100) }%, {loss_value=}, {eval_loss=}, {acc=}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7c9938ca-7f5d-419e-948b-b26304ad6838",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0.0%, loss_value=Array(0.1006145, dtype=float32), eval_loss=Array(0.09500747, dtype=float32), acc=Array(0.09922373, dtype=float32, weak_type=True)\n",
      "step=10.0%, loss_value=Array(0.02966868, dtype=float32), eval_loss=Array(0.03291377, dtype=float32), acc=Array(0.7995621, dtype=float32, weak_type=True)\n",
      "step=20.0%, loss_value=Array(0.03069584, dtype=float32), eval_loss=Array(0.02885927, dtype=float32), acc=Array(0.81041, dtype=float32, weak_type=True)\n",
      "step=30.0%, loss_value=Array(0.01746745, dtype=float32), eval_loss=Array(0.02669366, dtype=float32), acc=Array(0.82573646, dtype=float32, weak_type=True)\n",
      "step=40.0%, loss_value=Array(0.03112168, dtype=float32), eval_loss=Array(0.02632064, dtype=float32), acc=Array(0.8225517, dtype=float32, weak_type=True)\n",
      "step=50.0%, loss_value=Array(0.02785086, dtype=float32), eval_loss=Array(0.02399013, dtype=float32), acc=Array(0.83588773, dtype=float32, weak_type=True)\n",
      "step=59.0%, loss_value=Array(0.02864545, dtype=float32), eval_loss=Array(0.02533414, dtype=float32), acc=Array(0.82285035, dtype=float32, weak_type=True)\n",
      "step=69.0%, loss_value=Array(0.03246569, dtype=float32), eval_loss=Array(0.02232733, dtype=float32), acc=Array(0.84524286, dtype=float32, weak_type=True)\n",
      "step=79.0%, loss_value=Array(0.02188878, dtype=float32), eval_loss=Array(0.02177288, dtype=float32), acc=Array(0.8433519, dtype=float32, weak_type=True)\n",
      "step=89.0%, loss_value=Array(0.02437152, dtype=float32), eval_loss=Array(0.02211844, dtype=float32), acc=Array(0.8486266, dtype=float32, weak_type=True)\n",
      "step=99.0%, loss_value=Array(0.02168432, dtype=float32), eval_loss=Array(0.0207607, dtype=float32), acc=Array(0.85061705, dtype=float32, weak_type=True)\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = fit(model, trainloader, testloader, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d4a6e-f4fb-4620-b032-43cfe7c6d139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
