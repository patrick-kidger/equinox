{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d4b318-f46e-4d2c-92cd-632027a03632",
   "metadata": {},
   "source": [
    "# Train RNN\n",
    "\n",
    "This is an introductory example. We demonstrate what using Equinox normally looks like day-to-day.\n",
    "\n",
    "Here, we'll train an RNN to classify clockwise vs anticlockwise spirals.\n",
    "\n",
    "This example is available as a Jupyter notebook [here](https://github.com/patrick-kidger/equinox/blob/main/examples/train_rnn.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690edba4-62d8-48ff-8ed2-2dd322399525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax  # https://github.com/deepmind/optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5423ff-c18b-4394-9d1d-0bb4bfd626d6",
   "metadata": {},
   "source": [
    "We begin by importing the usual libraries, setting up a very simple dataloader, and generating a toy dataset of spirals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f40756-55d6-44f4-b13b-b15db35e15e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = np.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = np.random.permutation(indices)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end <= dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "\n",
    "\n",
    "def get_data(dataset_size, *, key):\n",
    "    t = jnp.linspace(0, 2 * math.pi, 16)\n",
    "    offset = jrandom.uniform(key, (dataset_size, 1), minval=0, maxval=2 * math.pi)\n",
    "    x1 = jnp.sin(t + offset) / (1 + t)\n",
    "    x2 = jnp.cos(t + offset) / (1 + t)\n",
    "    y = jnp.ones((dataset_size, 1))\n",
    "\n",
    "    half_dataset_size = dataset_size // 2\n",
    "    x1 = x1.at[:half_dataset_size].multiply(-1)\n",
    "    y = y.at[:half_dataset_size].set(0)\n",
    "    x = jnp.stack([x1, x2], axis=-1)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4094e1f-b1b5-4e3b-9191-bf09a5f93dda",
   "metadata": {},
   "source": [
    "Now for our model.\n",
    "\n",
    "Purely by way of example, we handle the final adding on of bias ourselves, rather than letting the `linear` layer do it. This is just so we can demonstrate how to use custom parameters in models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "533bdd6b-d660-4d94-bca9-da80ad0c0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(eqx.Module):\n",
    "    hidden_size: int\n",
    "    cell: eqx.Module\n",
    "    linear: eqx.nn.Linear\n",
    "    bias: jax.Array\n",
    "\n",
    "    def __init__(self, in_size, out_size, hidden_size, *, key):\n",
    "        ckey, lkey = jrandom.split(key)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = eqx.nn.GRUCell(in_size, hidden_size, key=ckey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, out_size, use_bias=False, key=lkey)\n",
    "        self.bias = jnp.zeros(out_size)\n",
    "\n",
    "    def __call__(self, input):\n",
    "        hidden = jnp.zeros((self.hidden_size,))\n",
    "\n",
    "        def f(carry, inp):\n",
    "            return self.cell(inp, carry), None\n",
    "\n",
    "        out, _ = lax.scan(f, hidden, input)\n",
    "        # sigmoid because we're performing binary classification\n",
    "        return jax.nn.sigmoid(self.linear(out) + self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb040df3-58c0-4546-b64d-c25147173e4b",
   "metadata": {},
   "source": [
    "And finally the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edef054d-5fdb-491e-8a46-c4d80363d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=10000,\n",
    "    batch_size=32,\n",
    "    learning_rate=3e-3,\n",
    "    steps=200,\n",
    "    hidden_size=16,\n",
    "    depth=1,\n",
    "    seed=5678,\n",
    "):\n",
    "    data_key, model_key = jrandom.split(jrandom.PRNGKey(seed), 2)\n",
    "    xs, ys = get_data(dataset_size, key=data_key)\n",
    "    iter_data = dataloader((xs, ys), batch_size)\n",
    "\n",
    "    model = RNN(in_size=2, out_size=1, hidden_size=hidden_size, key=model_key)\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def compute_loss(model, x, y):\n",
    "        pred_y = jax.vmap(model)(x)\n",
    "        # Trains with respect to binary cross-entropy\n",
    "        return -jnp.mean(y * jnp.log(pred_y) + (1 - y) * jnp.log(1 - pred_y))\n",
    "\n",
    "    # Important for efficiency whenever you use JAX: wrap everything into a single JIT\n",
    "    # region.\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, x, y, opt_state):\n",
    "        loss, grads = compute_loss(model, x, y)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return loss, model, opt_state\n",
    "\n",
    "    optim = optax.adam(learning_rate)\n",
    "    opt_state = optim.init(model)\n",
    "    for step, (x, y) in zip(range(steps), iter_data):\n",
    "        loss, model, opt_state = make_step(model, x, y, opt_state)\n",
    "        loss = loss.item()\n",
    "        print(f\"step={step}, loss={loss}\")\n",
    "\n",
    "    pred_ys = jax.vmap(model)(xs)\n",
    "    num_correct = jnp.sum((pred_ys > 0.5) == ys)\n",
    "    final_accuracy = (num_correct / dataset_size).item()\n",
    "    print(f\"final_accuracy={final_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6498efb9-f3d5-4dfb-b3f8-68f2b42422ec",
   "metadata": {},
   "source": [
    "`eqx.filter_value_and_grad` will calculate the gradient with respect to all floating-point arrays in the first argument (`model`). In this case the `model` parameters will be differentiated, whilst `model.hidden_size` is an integer and will get `None` as its gradient.\n",
    "\n",
    "Likewise, `eqx.filter_jit` will look at all the arguments passed to `make_step`, and automatically JIT-trace every array and JIT-static everything else. In this case the `model` parameters and the data `x` and `y` will be traced, whilst `model.hidden_size` is an integer and will be static'd instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c05289e-2bd9-4f69-8a57-62d2f2f8047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0, loss=0.6816999316215515\n",
      "step=1, loss=0.7202574014663696\n",
      "step=2, loss=0.6925007104873657\n",
      "step=3, loss=0.689198911190033\n",
      "step=4, loss=0.6808685064315796\n",
      "step=5, loss=0.7059305906295776\n",
      "step=6, loss=0.6922754049301147\n",
      "step=7, loss=0.6842439770698547\n",
      "step=8, loss=0.6972116231918335\n",
      "step=9, loss=0.7047306299209595\n",
      "step=10, loss=0.6993851661682129\n",
      "step=11, loss=0.6921849846839905\n",
      "step=12, loss=0.6844913959503174\n",
      "step=13, loss=0.6941200494766235\n",
      "step=14, loss=0.6870629787445068\n",
      "step=15, loss=0.6922240257263184\n",
      "step=16, loss=0.6966875195503235\n",
      "step=17, loss=0.7021255493164062\n",
      "step=18, loss=0.6913468241691589\n",
      "step=19, loss=0.6915531158447266\n",
      "step=20, loss=0.6906869411468506\n",
      "step=21, loss=0.6945821046829224\n",
      "step=22, loss=0.6963403820991516\n",
      "step=23, loss=0.6893304586410522\n",
      "step=24, loss=0.6923031210899353\n",
      "step=25, loss=0.6952496767044067\n",
      "step=26, loss=0.6937462687492371\n",
      "step=27, loss=0.6946915984153748\n",
      "step=28, loss=0.6912715435028076\n",
      "step=29, loss=0.6945470571517944\n",
      "step=30, loss=0.6928573250770569\n",
      "step=31, loss=0.6918295621871948\n",
      "step=32, loss=0.6926039457321167\n",
      "step=33, loss=0.691811203956604\n",
      "step=34, loss=0.696336567401886\n",
      "step=35, loss=0.693527340888977\n",
      "step=36, loss=0.6909832954406738\n",
      "step=37, loss=0.6898350715637207\n",
      "step=38, loss=0.693118691444397\n",
      "step=39, loss=0.6962690353393555\n",
      "step=40, loss=0.6943768262863159\n",
      "step=41, loss=0.6929119229316711\n",
      "step=42, loss=0.6921533942222595\n",
      "step=43, loss=0.6970506906509399\n",
      "step=44, loss=0.6914128065109253\n",
      "step=45, loss=0.6925110220909119\n",
      "step=46, loss=0.6876767873764038\n",
      "step=47, loss=0.6977562308311462\n",
      "step=48, loss=0.6887734532356262\n",
      "step=49, loss=0.6956733465194702\n",
      "step=50, loss=0.6988524198532104\n",
      "step=51, loss=0.6972949504852295\n",
      "step=52, loss=0.6935367584228516\n",
      "step=53, loss=0.6899304389953613\n",
      "step=54, loss=0.6940433979034424\n",
      "step=55, loss=0.6932569742202759\n",
      "step=56, loss=0.6964170932769775\n",
      "step=57, loss=0.6952816843986511\n",
      "step=58, loss=0.6925933361053467\n",
      "step=59, loss=0.700016975402832\n",
      "step=60, loss=0.6929588317871094\n",
      "step=61, loss=0.6919406652450562\n",
      "step=62, loss=0.6893216371536255\n",
      "step=63, loss=0.6881398558616638\n",
      "step=64, loss=0.6941375136375427\n",
      "step=65, loss=0.6908596754074097\n",
      "step=66, loss=0.6938614845275879\n",
      "step=67, loss=0.6939255595207214\n",
      "step=68, loss=0.691447377204895\n",
      "step=69, loss=0.6932423114776611\n",
      "step=70, loss=0.6937750577926636\n",
      "step=71, loss=0.691257119178772\n",
      "step=72, loss=0.6900532245635986\n",
      "step=73, loss=0.6922309398651123\n",
      "step=74, loss=0.6899502277374268\n",
      "step=75, loss=0.6930654048919678\n",
      "step=76, loss=0.6942011117935181\n",
      "step=77, loss=0.6899413466453552\n",
      "step=78, loss=0.6950610876083374\n",
      "step=79, loss=0.6900242567062378\n",
      "step=80, loss=0.691747784614563\n",
      "step=81, loss=0.6899303793907166\n",
      "step=82, loss=0.6910462379455566\n",
      "step=83, loss=0.69475257396698\n",
      "step=84, loss=0.6886341571807861\n",
      "step=85, loss=0.6912660598754883\n",
      "step=86, loss=0.6889529824256897\n",
      "step=87, loss=0.6940121054649353\n",
      "step=88, loss=0.6970347762107849\n",
      "step=89, loss=0.687224268913269\n",
      "step=90, loss=0.6900577545166016\n",
      "step=91, loss=0.6913183927536011\n",
      "step=92, loss=0.6916753649711609\n",
      "step=93, loss=0.6899659633636475\n",
      "step=94, loss=0.6911211013793945\n",
      "step=95, loss=0.694290041923523\n",
      "step=96, loss=0.7031664848327637\n",
      "step=97, loss=0.6912339925765991\n",
      "step=98, loss=0.6968348026275635\n",
      "step=99, loss=0.6970176100730896\n",
      "step=100, loss=0.6857004165649414\n",
      "step=101, loss=0.6842451095581055\n",
      "step=102, loss=0.6882964968681335\n",
      "step=103, loss=0.6855384111404419\n",
      "step=104, loss=0.6909692287445068\n",
      "step=105, loss=0.6905874013900757\n",
      "step=106, loss=0.6900045871734619\n",
      "step=107, loss=0.6865564584732056\n",
      "step=108, loss=0.6820229887962341\n",
      "step=109, loss=0.6879786849021912\n",
      "step=110, loss=0.6853011846542358\n",
      "step=111, loss=0.68475741147995\n",
      "step=112, loss=0.682267427444458\n",
      "step=113, loss=0.6880433559417725\n",
      "step=114, loss=0.6814002990722656\n",
      "step=115, loss=0.6823583841323853\n",
      "step=116, loss=0.6794727444648743\n",
      "step=117, loss=0.6785068511962891\n",
      "step=118, loss=0.6811013221740723\n",
      "step=119, loss=0.6747442483901978\n",
      "step=120, loss=0.6660218238830566\n",
      "step=121, loss=0.6700407266616821\n",
      "step=122, loss=0.6526561975479126\n",
      "step=123, loss=0.6608943939208984\n",
      "step=124, loss=0.6293025612831116\n",
      "step=125, loss=0.6483496427536011\n",
      "step=126, loss=0.6219364404678345\n",
      "step=127, loss=0.5961954593658447\n",
      "step=128, loss=0.6002600193023682\n",
      "step=129, loss=0.5647848844528198\n",
      "step=130, loss=0.5256890058517456\n",
      "step=131, loss=0.510317325592041\n",
      "step=132, loss=0.47984960675239563\n",
      "step=133, loss=0.5084915161132812\n",
      "step=134, loss=0.4301827549934387\n",
      "step=135, loss=0.4290550649166107\n",
      "step=136, loss=0.3755859136581421\n",
      "step=137, loss=0.2937808036804199\n",
      "step=138, loss=0.26023393869400024\n",
      "step=139, loss=0.23048073053359985\n",
      "step=140, loss=0.21439003944396973\n",
      "step=141, loss=0.1652923822402954\n",
      "step=142, loss=0.1283920854330063\n",
      "step=143, loss=0.10732141137123108\n",
      "step=144, loss=0.09533026814460754\n",
      "step=145, loss=0.0801059827208519\n",
      "step=146, loss=0.06879423558712006\n",
      "step=147, loss=0.05884774401783943\n",
      "step=148, loss=0.04997169226408005\n",
      "step=149, loss=0.04442397877573967\n",
      "step=150, loss=0.03894098848104477\n",
      "step=151, loss=0.03257341682910919\n",
      "step=152, loss=0.029269497841596603\n",
      "step=153, loss=0.025602124631404877\n",
      "step=154, loss=0.022208761423826218\n",
      "step=155, loss=0.019779304042458534\n",
      "step=156, loss=0.0190683975815773\n",
      "step=157, loss=0.01669464260339737\n",
      "step=158, loss=0.015229889191687107\n",
      "step=159, loss=0.013572589494287968\n",
      "step=160, loss=0.013138247653841972\n",
      "step=161, loss=0.011538836173713207\n",
      "step=162, loss=0.011130412109196186\n",
      "step=163, loss=0.010209350846707821\n",
      "step=164, loss=0.00980051327496767\n",
      "step=165, loss=0.009073751047253609\n",
      "step=166, loss=0.008881419897079468\n",
      "step=167, loss=0.008286576718091965\n",
      "step=168, loss=0.007832735776901245\n",
      "step=169, loss=0.007236967794597149\n",
      "step=170, loss=0.006913297809660435\n",
      "step=171, loss=0.006962948478758335\n",
      "step=172, loss=0.0067381164990365505\n",
      "step=173, loss=0.0065834359265863895\n",
      "step=174, loss=0.006057928316295147\n",
      "step=175, loss=0.006013630423694849\n",
      "step=176, loss=0.005771873984485865\n",
      "step=177, loss=0.005476493388414383\n",
      "step=178, loss=0.005490635521709919\n",
      "step=179, loss=0.0053573474287986755\n",
      "step=180, loss=0.005200999788939953\n",
      "step=181, loss=0.005007486790418625\n",
      "step=182, loss=0.004953362978994846\n",
      "step=183, loss=0.004767540842294693\n",
      "step=184, loss=0.004584995098412037\n",
      "step=185, loss=0.004679872654378414\n",
      "step=186, loss=0.004541940055787563\n",
      "step=187, loss=0.004449024796485901\n",
      "step=188, loss=0.004398198798298836\n",
      "step=189, loss=0.004358283709734678\n",
      "step=190, loss=0.004335914738476276\n",
      "step=191, loss=0.004185144789516926\n",
      "step=192, loss=0.00412558950483799\n",
      "step=193, loss=0.0040787700563669205\n",
      "step=194, loss=0.003923933021724224\n",
      "step=195, loss=0.0039162710309028625\n",
      "step=196, loss=0.0038443428929895163\n",
      "step=197, loss=0.0037653278559446335\n",
      "step=198, loss=0.0037484394852072\n",
      "step=199, loss=0.0037215303163975477\n",
      "final_accuracy=1.0\n"
     ]
    }
   ],
   "source": [
    "main()  # All right, let's run the code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax39",
   "language": "python",
   "name": "jax39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
