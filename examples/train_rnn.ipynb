{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d4b318-f46e-4d2c-92cd-632027a03632",
   "metadata": {},
   "source": [
    "# Train RNN\n",
    "\n",
    "Here we give a complete example of what using Equinox normally looks like day-to-day.\n",
    "\n",
    "In this example we'll train an RNN to classify clockwise vs anticlockwise spirals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690edba4-62d8-48ff-8ed2-2dd322399525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "\n",
    "import equinox as eqx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5423ff-c18b-4394-9d1d-0bb4bfd626d6",
   "metadata": {},
   "source": [
    "We begin by importing the usual libraries, setting up a very simple dataloader, and generating a toy dataset of spirals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f40756-55d6-44f4-b13b-b15db35e15e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "\n",
    "\n",
    "def get_data(dataset_size, *, key):\n",
    "    t = jnp.linspace(0, 2 * math.pi, 16)\n",
    "    offset = jrandom.uniform(key, (dataset_size, 1), minval=0, maxval=2 * math.pi)\n",
    "    x1 = jnp.sin(t + offset) / (1 + t)\n",
    "    x2 = jnp.cos(t + offset) / (1 + t)\n",
    "    y = jnp.ones((dataset_size, 1))\n",
    "\n",
    "    half_dataset_size = dataset_size // 2\n",
    "    x1 = x1.at[:half_dataset_size].multiply(-1)\n",
    "    y = y.at[:half_dataset_size].set(0)\n",
    "    x = jnp.stack([x1, x2], axis=-1)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4094e1f-b1b5-4e3b-9191-bf09a5f93dda",
   "metadata": {},
   "source": [
    "Now for our model.\n",
    "\n",
    "Purely by way of example, we handle the final adding on of bias ourselves, rather than letting the `linear` layer do it. This is just so we can demonstrate how to use custom parameters in models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "533bdd6b-d660-4d94-bca9-da80ad0c0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(eqx.Module):\n",
    "    hidden_size: int\n",
    "    cell: eqx.Module\n",
    "    linear: eqx.nn.Linear\n",
    "    bias: jnp.ndarray\n",
    "\n",
    "    def __init__(self, in_size, out_size, hidden_size, *, key):\n",
    "        ckey, lkey = jrandom.split(key)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = eqx.nn.GRUCell(in_size, hidden_size, key=ckey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, out_size, use_bias=False, key=lkey)\n",
    "        self.bias = jnp.zeros(out_size)\n",
    "\n",
    "    def __call__(self, input):\n",
    "        hidden = jnp.zeros((self.hidden_size,))\n",
    "\n",
    "        def f(carry, inp):\n",
    "            return self.cell(inp, carry), None\n",
    "\n",
    "        out, _ = lax.scan(f, hidden, input)\n",
    "        # sigmoid because we're performing binary classification\n",
    "        return jax.nn.sigmoid(self.linear(out) + self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb040df3-58c0-4546-b64d-c25147173e4b",
   "metadata": {},
   "source": [
    "And finally the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edef054d-5fdb-491e-8a46-c4d80363d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=10000,\n",
    "    batch_size=32,\n",
    "    learning_rate=3e-3,\n",
    "    steps=200,\n",
    "    hidden_size=16,\n",
    "    depth=1,\n",
    "    seed=5678,\n",
    "):\n",
    "    data_key, loader_key, model_key = jrandom.split(jrandom.PRNGKey(seed), 3)\n",
    "    xs, ys = get_data(dataset_size, key=data_key)\n",
    "    iter_data = dataloader((xs, ys), batch_size, key=loader_key)\n",
    "\n",
    "    model = RNN(in_size=2, out_size=1, hidden_size=hidden_size, key=model_key)\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def compute_loss(model, x, y):\n",
    "        pred_y = jax.vmap(model)(x)\n",
    "        # Trains with respect to binary cross-entropy\n",
    "        return -jnp.mean(y * jnp.log(pred_y) + (1 - y) * jnp.log(1 - pred_y))\n",
    "\n",
    "    # Important for efficiency whenever you use JAX: wrap everything into a single JIT\n",
    "    # region.\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, x, y, opt_state):\n",
    "        loss, grads = compute_loss(model, x, y)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return loss, model, opt_state\n",
    "\n",
    "    optim = optax.adam(learning_rate)\n",
    "    opt_state = optim.init(model)\n",
    "    for step, (x, y) in zip(range(steps), iter_data):\n",
    "        loss, model, opt_state = make_step(model, x, y, opt_state)\n",
    "        loss = loss.item()\n",
    "        print(f\"step={step}, loss={loss}\")\n",
    "\n",
    "    pred_ys = jax.vmap(model)(xs)\n",
    "    num_correct = jnp.sum((pred_ys > 0.5) == ys)\n",
    "    final_accuracy = (num_correct / dataset_size).item()\n",
    "    print(f\"final_accuracy={final_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6498efb9-f3d5-4dfb-b3f8-68f2b42422ec",
   "metadata": {},
   "source": [
    "`eqx.filter_value_and_grad` will calculate the gradient with respect to the first argument (`model`). By default it will calculate gradients for all the floating-point JAX arrays and ignore everything else. For example the `model` parameters will be differentiated, whilst `model.hidden_size` is an integer and will be left alone. If you need finer control then these defaults can be adjusted; see [`equinox.filter_grad`][] and [`equinox.filter_value_and_grad`][].\n",
    "\n",
    "Likewise, by default, `eqx.filter_jit` will look at all the arguments passed to `make_step`, and automatically JIT-trace every array and JIT-static everything else. For example the `model` parameters and the data `x` and `y` will be traced, whilst `model.hidden_size` is an integer and will be static'd instead. Once again if you need finer control then these defaults can be adjusted; see [`equinox.filter_jit`][]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c05289e-2bd9-4f69-8a57-62d2f2f8047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0, loss=0.6968629360198975\n",
      "step=1, loss=0.6928662657737732\n",
      "step=2, loss=0.6924071311950684\n",
      "step=3, loss=0.6936538219451904\n",
      "step=4, loss=0.6929900050163269\n",
      "step=5, loss=0.6925624012947083\n",
      "step=6, loss=0.6939840912818909\n",
      "step=7, loss=0.6934851408004761\n",
      "step=8, loss=0.6915387511253357\n",
      "step=9, loss=0.6939077973365784\n",
      "step=10, loss=0.691526472568512\n",
      "step=11, loss=0.6946976184844971\n",
      "step=12, loss=0.6928755640983582\n",
      "step=13, loss=0.6925874948501587\n",
      "step=14, loss=0.6944625377655029\n",
      "step=15, loss=0.6954206228256226\n",
      "step=16, loss=0.6910819411277771\n",
      "step=17, loss=0.6946226358413696\n",
      "step=18, loss=0.6907551288604736\n",
      "step=19, loss=0.6943591833114624\n",
      "step=20, loss=0.6935365200042725\n",
      "step=21, loss=0.6931085586547852\n",
      "step=22, loss=0.6927573680877686\n",
      "step=23, loss=0.6918144226074219\n",
      "step=24, loss=0.693355917930603\n",
      "step=25, loss=0.6934046745300293\n",
      "step=26, loss=0.6928523778915405\n",
      "step=27, loss=0.6927888989448547\n",
      "step=28, loss=0.6911575794219971\n",
      "step=29, loss=0.6926875114440918\n",
      "step=30, loss=0.6926926374435425\n",
      "step=31, loss=0.6927932500839233\n",
      "step=32, loss=0.6920660734176636\n",
      "step=33, loss=0.6931584477424622\n",
      "step=34, loss=0.6925565004348755\n",
      "step=35, loss=0.6932635307312012\n",
      "step=36, loss=0.6928280591964722\n",
      "step=37, loss=0.6931933164596558\n",
      "step=38, loss=0.6919360160827637\n",
      "step=39, loss=0.6913033723831177\n",
      "step=40, loss=0.6925539970397949\n",
      "step=41, loss=0.6936467885971069\n",
      "step=42, loss=0.6933906078338623\n",
      "step=43, loss=0.6905258893966675\n",
      "step=44, loss=0.693335235118866\n",
      "step=45, loss=0.6936687231063843\n",
      "step=46, loss=0.6922796964645386\n",
      "step=47, loss=0.6942081451416016\n",
      "step=48, loss=0.6924135684967041\n",
      "step=49, loss=0.6919693946838379\n",
      "step=50, loss=0.6911320686340332\n",
      "step=51, loss=0.6917257308959961\n",
      "step=52, loss=0.6902439594268799\n",
      "step=53, loss=0.6989374756813049\n",
      "step=54, loss=0.6880578994750977\n",
      "step=55, loss=0.6932367086410522\n",
      "step=56, loss=0.6903895139694214\n",
      "step=57, loss=0.6951816082000732\n",
      "step=58, loss=0.6881908774375916\n",
      "step=59, loss=0.6912969350814819\n",
      "step=60, loss=0.692997395992279\n",
      "step=61, loss=0.6937721967697144\n",
      "step=62, loss=0.6948648691177368\n",
      "step=63, loss=0.6913964748382568\n",
      "step=64, loss=0.6929829120635986\n",
      "step=65, loss=0.6920725703239441\n",
      "step=66, loss=0.6956514120101929\n",
      "step=67, loss=0.6899707317352295\n",
      "step=68, loss=0.6912500858306885\n",
      "step=69, loss=0.690929114818573\n",
      "step=70, loss=0.690754771232605\n",
      "step=71, loss=0.6868723630905151\n",
      "step=72, loss=0.6915323138237\n",
      "step=73, loss=0.6870177984237671\n",
      "step=74, loss=0.6885477304458618\n",
      "step=75, loss=0.6867898106575012\n",
      "step=76, loss=0.68553626537323\n",
      "step=77, loss=0.6821398138999939\n",
      "step=78, loss=0.6817562580108643\n",
      "step=79, loss=0.6883955597877502\n",
      "step=80, loss=0.6734539270401001\n",
      "step=81, loss=0.673201322555542\n",
      "step=82, loss=0.6518104076385498\n",
      "step=83, loss=0.6409814357757568\n",
      "step=84, loss=0.6470382213592529\n",
      "step=85, loss=0.6248712539672852\n",
      "step=86, loss=0.5959113240242004\n",
      "step=87, loss=0.5411312580108643\n",
      "step=88, loss=0.5192182064056396\n",
      "step=89, loss=0.4482913017272949\n",
      "step=90, loss=0.42512020468711853\n",
      "step=91, loss=0.39694473147392273\n",
      "step=92, loss=0.31162846088409424\n",
      "step=93, loss=0.3591296076774597\n",
      "step=94, loss=0.3391856551170349\n",
      "step=95, loss=0.22171661257743835\n",
      "step=96, loss=0.13305065035820007\n",
      "step=97, loss=0.1562710404396057\n",
      "step=98, loss=0.12676352262496948\n",
      "step=99, loss=0.13090220093727112\n",
      "step=100, loss=0.08012909442186356\n",
      "step=101, loss=0.07420005649328232\n",
      "step=102, loss=0.06451655924320221\n",
      "step=103, loss=0.0709628015756607\n",
      "step=104, loss=0.04913963004946709\n",
      "step=105, loss=0.03378813713788986\n",
      "step=106, loss=0.03226414695382118\n",
      "step=107, loss=0.024383708834648132\n",
      "step=108, loss=0.02412772923707962\n",
      "step=109, loss=0.015441099181771278\n",
      "step=110, loss=0.014490555040538311\n",
      "step=111, loss=0.012725356966257095\n",
      "step=112, loss=0.011480225250124931\n",
      "step=113, loss=0.01063367910683155\n",
      "step=114, loss=0.009327923879027367\n",
      "step=115, loss=0.008870435878634453\n",
      "step=116, loss=0.008231064304709435\n",
      "step=117, loss=0.007535358890891075\n",
      "step=118, loss=0.007148533593863249\n",
      "step=119, loss=0.0070632947608828545\n",
      "step=120, loss=0.00653859693557024\n",
      "step=121, loss=0.0059097642078995705\n",
      "step=122, loss=0.005809098947793245\n",
      "step=123, loss=0.005686894059181213\n",
      "step=124, loss=0.005436614155769348\n",
      "step=125, loss=0.005461064167320728\n",
      "step=126, loss=0.005191616714000702\n",
      "step=127, loss=0.005134109407663345\n",
      "step=128, loss=0.004667493049055338\n",
      "step=129, loss=0.004276127088814974\n",
      "step=130, loss=0.004375998862087727\n",
      "step=131, loss=0.0046746088191866875\n",
      "step=132, loss=0.0045977989211678505\n",
      "step=133, loss=0.004235218744724989\n",
      "step=134, loss=0.004297919105738401\n",
      "step=135, loss=0.004085398279130459\n",
      "step=136, loss=0.003860234282910824\n",
      "step=137, loss=0.0037155605386942625\n",
      "step=138, loss=0.003936016000807285\n",
      "step=139, loss=0.0038189340848475695\n",
      "step=140, loss=0.0035462528467178345\n",
      "step=141, loss=0.003776055295020342\n",
      "step=142, loss=0.0036001354455947876\n",
      "step=143, loss=0.003455652855336666\n",
      "step=144, loss=0.003320368705317378\n",
      "step=145, loss=0.0033631217665970325\n",
      "step=146, loss=0.003434734884649515\n",
      "step=147, loss=0.0035251914523541927\n",
      "step=148, loss=0.0030395472422242165\n",
      "step=149, loss=0.003123756032437086\n",
      "step=150, loss=0.0030222469940781593\n",
      "step=151, loss=0.0030912940856069326\n",
      "step=152, loss=0.0030278991907835007\n",
      "step=153, loss=0.0028344655875116587\n",
      "step=154, loss=0.0028626525308936834\n",
      "step=155, loss=0.0028719506226480007\n",
      "step=156, loss=0.002740566385909915\n",
      "step=157, loss=0.002740482334047556\n",
      "step=158, loss=0.0027753664180636406\n",
      "step=159, loss=0.002754327142611146\n",
      "step=160, loss=0.0026870560832321644\n",
      "step=161, loss=0.0028148717246949673\n",
      "step=162, loss=0.0027450912166386843\n",
      "step=163, loss=0.0025830306112766266\n",
      "step=164, loss=0.0025843908078968525\n",
      "step=165, loss=0.002580533269792795\n",
      "step=166, loss=0.0026080277748405933\n",
      "step=167, loss=0.0025642546825110912\n",
      "step=168, loss=0.0023213259410113096\n",
      "step=169, loss=0.00243693171069026\n",
      "step=170, loss=0.0025095257442444563\n",
      "step=171, loss=0.00239811884239316\n",
      "step=172, loss=0.002309786155819893\n",
      "step=173, loss=0.0023783245123922825\n",
      "step=174, loss=0.002440847223624587\n",
      "step=175, loss=0.0023895162157714367\n",
      "step=176, loss=0.002205097349360585\n",
      "step=177, loss=0.002265633549541235\n",
      "step=178, loss=0.0022901236079633236\n",
      "step=179, loss=0.0022574211470782757\n",
      "step=180, loss=0.0021944043692201376\n",
      "step=181, loss=0.0021767830476164818\n",
      "step=182, loss=0.002231168793514371\n",
      "step=183, loss=0.0021353098563849926\n",
      "step=184, loss=0.0021301782689988613\n",
      "step=185, loss=0.002177216112613678\n",
      "step=186, loss=0.0021116407588124275\n",
      "step=187, loss=0.0021116193383932114\n",
      "step=188, loss=0.002098802709951997\n",
      "step=189, loss=0.002040596678853035\n",
      "step=190, loss=0.0019703381694853306\n",
      "step=191, loss=0.002052887110039592\n",
      "step=192, loss=0.002063254825770855\n",
      "step=193, loss=0.0020025877747684717\n",
      "step=194, loss=0.0019500794587656856\n",
      "step=195, loss=0.002010410651564598\n",
      "step=196, loss=0.00195988523773849\n",
      "step=197, loss=0.0019289047922939062\n",
      "step=198, loss=0.0019055064767599106\n",
      "step=199, loss=0.0019205857533961535\n",
      "final_accuracy=1.0\n"
     ]
    }
   ],
   "source": [
    "main()  # All right, let's run the code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax030-310",
   "language": "python",
   "name": "jax030-310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
