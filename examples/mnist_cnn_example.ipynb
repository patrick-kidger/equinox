{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c323c856-04c5-40fe-8dae-cdab30144eef",
   "metadata": {},
   "source": [
    "# A Simple Convolutional Neural Network on the MNIST Dataset\n",
    "\n",
    "In this example, we will implement a CNN and this should teach you the following important concepts:\n",
    "\n",
    "    - How to create a custom neural network\n",
    "    - Why and when it makes sense to use exq.filter_[...] functions\n",
    "    - How to train your network using Optax\n",
    "    - What your Neural Network looks like \"under the hood\" (foreshadowing: like a PyTree!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8b9ef-2a4b-4186-b016-77257d8387e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all the necessary libraries\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from jaxtyping import PyTree\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import equinox as eqx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf96510-c313-451c-8764-6c3a975950a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca61991-c7e4-4fec-aad5-0f91958dc91a",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "For this example, we will use the *regular* MNIST dataset. To load it up, we use PyTorch and its excellent DataLoader class. To use it freely, we will need to\n",
    "    1. normalise the data\n",
    "    2. one-hot encode the labels\n",
    "\n",
    "The first step, we can perform using some PyTorch utilities, while for the second step, we can use Jax's `one_hot` function.\n",
    "\n",
    "You can check the [PyTorch documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for more information on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265bf49-5ffb-40df-b89e-d2b411c53e35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(label):\n",
    "    \"\"\"\n",
    "    This function one-hot encodes the target label.\n",
    "    E.g.:\n",
    "        label = 3\n",
    "        output = [0 0 0 1 0 0 0 0 0 0]\n",
    "    \"\"\"\n",
    "    return np.array(jax.nn.one_hot(label, 10))\n",
    "\n",
    "\n",
    "# transforms\n",
    "# This normalises the data\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "\n",
    "# Load the MNIST train data\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=one_hot_encode,\n",
    ")\n",
    "\n",
    "# Load the MNIST test data\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=one_hot_encode,\n",
    ")\n",
    "\n",
    "# Create the train and testloaders\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd38e98-187d-458b-be5a-e6922c9d996a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking our data a bit (by now, everyone knows what the MNIST dataset looks like)\n",
    "dummy_x, dummy_y = next(iter(trainloader))\n",
    "dummy_x = dummy_x.numpy()\n",
    "dummy_y = dummy_y.numpy()\n",
    "print(dummy_x.shape)  # 64x1x28x28\n",
    "print(dummy_y.shape)  # 64x10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9f79e2-9631-4974-bb12-11f90faa3d93",
   "metadata": {},
   "source": [
    "We can see that our data has the shape 64x1x28x28. 64 is the batch size, 1 is the number of input channels (this will be important soon) and 28x28 is the image dimension.\n",
    "\n",
    "## The Network\n",
    "\n",
    "Our Convolutional Neural Network (CNN) will have a list that contains all our layers. There is no explicit requirement to do it that way, it's simply convinient for this example. Inbetween the layers (which in essence are just Jax arrays, i.e. **leaves** of a PyTree), we have some things which are **not** arrays! These functions, e.g. `relu`, are not things that Jax can differentiate! This means we will have to *filter* those things out. Luckily, Equinox provides several useful filter functions for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67244a-9093-4fe3-a110-99b03354e63b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, key):\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "        # A simple neural network with 3 hidden layers\n",
    "        self.layers = [\n",
    "            eqx.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=4, key=key1),\n",
    "            eqx.nn.MaxPool2d(kernel_size=2),\n",
    "            jax.nn.relu,\n",
    "            jnp.ravel,\n",
    "            eqx.nn.Linear(1728, 512, key=key2),\n",
    "            jax.nn.sigmoid,\n",
    "            eqx.nn.Linear(512, 64, key=key3),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(64, 10, key=key4),\n",
    "            jax.nn.softmax,\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            # call relu on each layer\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de352850-b04f-41cd-b178-7a0e829f6db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(model, x, y):\n",
    "    # Our input has the shape (BATCH_SIZE x 1 x 28 x 28) but our model expects an\n",
    "    # input image with a single input channel;\n",
    "    # the batch dimension was not defined in our model!\n",
    "    # Therefore, we have to use jax.vmap, which - by default - maps over the 0th axis\n",
    "    # (i.e. the batch in our example)\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    # mean squared error loss\n",
    "    return jnp.mean((y - pred_y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c20d9bf-7954-4132-88ac-661239200747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Init our model\n",
    "key, subkey = jax.random.split(jax.random.PRNGKey(32))\n",
    "model = NeuralNetwork(subkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a98b74e-b049-4c71-9ffe-4c123ab96e10",
   "metadata": {},
   "source": [
    "### Our PyTree Model\n",
    "\n",
    "We can check that our model really is just a PyTree by using the `isinstance` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b55de8-a0db-4749-9fc9-e301646ac40a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "isinstance(model, PyTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0375ea-65af-4c99-881c-7f105ae452a2",
   "metadata": {},
   "source": [
    "And when we *pretty print* our model, we can see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18958774-ac0f-4eb1-bbb7-69764d7fd87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eqx.tree_pprint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec7f8db-bb08-434f-a24e-f5ccbdfa5b61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example inference\n",
    "output = jax.vmap(model)(dummy_x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99629b5-e96f-43fa-9e9e-29cbc9da8d4a",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "In the next cells we can see an example of when we should use the filter methods provided by Equinox. For instance, the following code generates an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719f539-2d5f-460c-8d70-47a721bd414e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is an error!\n",
    "\n",
    "jax.value_and_grad(loss)(model, dummy_x, dummy_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f058f290-d869-43fc-a414-d06295179625",
   "metadata": {},
   "source": [
    "And this makes sense, because when you look back up in the PyTree, you will see some lines that say e.g. `<wrapped function relu>,` . This means there is a function in our PyTree. When Jax tries to compute the gradient, it arrives at this function and is confused (rightfully so!). Therefore, we have to filter out anything that is not a differentiable PyTree leaf! \n",
    "\n",
    "A general rule of thumb is: since `eqx.filter_[...]` is essentially the same as the corresponding Jax functions, whenever you're unsure which one to use, you can simply fall back to the Equinox filter functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de35485-c4ef-4ec8-aa4e-b7da79c5c4bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This will work!\n",
    "\n",
    "# Getting the initial loss value\n",
    "value, grads = eqx.filter_value_and_grad(loss)(model, dummy_x, dummy_y)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54927e9e-af16-4480-a502-5b5cadd2d881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, testloader: DataLoader):\n",
    "    \"\"\"\n",
    "    This function evaluates the model on the testing dataset\n",
    "    by computing the average loss and also the average accuracy.\n",
    "    \"\"\"\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    jitted_loss = eqx.filter_jit(loss)\n",
    "    for x, y in testloader:\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        avg_loss += jitted_loss(model, x, y)\n",
    "        preds = jax.vmap(model)(x)\n",
    "        targets = jnp.argmax(y, axis=1)\n",
    "        preds = jnp.argmax(preds, axis=1)\n",
    "        avg_acc += sum(targets == preds) / len(x)\n",
    "    return avg_loss / len(testloader), avg_acc / len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5369a-7f25-42f7-96c0-12447a482acc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate(model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebdc6b2-a001-41e7-869c-6b151826709c",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now it's time to train our model using Optax! The most important thing to notice here is the `eqx.apply_updates` function! This is a wrapper for the function `optax.apply_updates`. But why do we need it? The reason is that `eqx.filter_value_and_grad` filters our anything that is not differentiable, e.g. functions in our PyTree and instead replaces those fields with `None`. This means that when you compute the gradients and the updates (from `optim.updates(...)`) there will be places with `None`s. The *original* function from Optax would then try to update `None`s using something like `None - learning_rate * None`, which is a big *no no*!\n",
    "\n",
    "Therefore we can use the utility function from Equinox, which has the identical functionality as `optax.apply_updates` only that it skips the `None`s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2123c-ad60-4318-8a40-79731f067da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use Optax to optimise the parameters\n",
    "optim = optax.adamw(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff12310-ce7b-4eb7-8910-84349afe8e93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(model, trainloader: DataLoader, testloader: DataLoader, optim):\n",
    "    # Get the optimiser state\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def step(model, opt_state, x, y):\n",
    "        loss_value, grads = eqx.filter_value_and_grad(loss)(model, x, y)\n",
    "        updates, opt_state = optim.update(grads, opt_state, model)\n",
    "        # We use eqx.apply_updates because our gradients contain Nones,\n",
    "        # which Optax can't handle by default\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss_value\n",
    "\n",
    "    for i, (x, y) in enumerate(trainloader):\n",
    "        # PyTorch dataloaders give torch tensors by default,\n",
    "        # so we have to convert them to Numpy arrays first\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        model, opt_state, loss_value = step(model, opt_state, x, y)\n",
    "        # Every 10% of the dataset, print a snapshot of the model's performance\n",
    "        if i % (len(trainloader) // 10) == 0:\n",
    "            eval_loss, acc = evaluate(model, testloader)\n",
    "            current_percentage = jnp.round((i / len(trainloader)) * 100)\n",
    "            print(f\"step={current_percentage}%, {loss_value=}, {eval_loss=}, {acc=}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab00c7f-012a-4ed3-bbe2-2a75a2bdd723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = fit(model, trainloader, testloader, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a9f2d7-b2d0-427e-8958-d0a381edc333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
