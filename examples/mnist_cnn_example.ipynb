{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c323c856-04c5-40fe-8dae-cdab30144eef",
   "metadata": {},
   "source": [
    "# A Simple Convolutional Neural Network on the MNIST Dataset\n",
    "\n",
    "In this example, we will implement a CNN and this should teach you the following important concepts:\n",
    "\n",
    "    - How to create a custom neural network\n",
    "    - Why and when it makes sense to use exq.filter_[...] functions\n",
    "    - How to train your network using Optax\n",
    "    - What your Neural Network looks like \"under the hood\" (foreshadowing: like a PyTree!)\n",
    "\n",
    "We also use [jaxtyping](https://github.com/google/jaxtyping) for type annotations. This makes it easier to collaborators to work on your code and also improves code quality while also reducing the probability of unexpected bugs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a8b9ef-2a4b-4186-b016-77257d8387e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all the necessary libraries\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from jaxtyping import Array, PyTree\n",
    "from optax import GradientTransformation\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import equinox as eqx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdf96510-c313-451c-8764-6c3a975950a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca61991-c7e4-4fec-aad5-0f91958dc91a",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "For this example, we will use the *regular* MNIST dataset. To load it up, we use PyTorch and its excellent DataLoader class. To use it freely, we will need to\n",
    "    1. normalise the data\n",
    "    2. one-hot encode the labels\n",
    "\n",
    "The first step, we can perform using some PyTorch utilities, while for the second step, we can use Jax's `one_hot` function.\n",
    "\n",
    "You can check the [PyTorch documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for more information on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7265bf49-5ffb-40df-b89e-d2b411c53e35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(label):\n",
    "    \"\"\"\n",
    "    This function one-hot encodes the target label.\n",
    "    E.g.:\n",
    "        label = 3\n",
    "        output = [0 0 0 1 0 0 0 0 0 0]\n",
    "    \"\"\"\n",
    "    return np.array(jax.nn.one_hot(label, 10))\n",
    "\n",
    "\n",
    "# transforms\n",
    "# This normalises the data\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "\n",
    "# Load the MNIST train data\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=one_hot_encode,\n",
    ")\n",
    "\n",
    "# Load the MNIST test data\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=one_hot_encode,\n",
    ")\n",
    "\n",
    "# Create the train and testloaders\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd38e98-187d-458b-be5a-e6922c9d996a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 28, 28)\n",
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "# Checking our data a bit (by now, everyone knows what the MNIST dataset looks like)\n",
    "dummy_x, dummy_y = next(iter(trainloader))\n",
    "dummy_x = dummy_x.numpy()\n",
    "dummy_y = dummy_y.numpy()\n",
    "print(dummy_x.shape)  # 64x1x28x28\n",
    "print(dummy_y.shape)  # 64x10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9f79e2-9631-4974-bb12-11f90faa3d93",
   "metadata": {},
   "source": [
    "We can see that our data has the shape 64x1x28x28. 64 is the batch size, 1 is the number of input channels (this will be important soon) and 28x28 is the image dimension.\n",
    "\n",
    "## The Network\n",
    "\n",
    "Our Convolutional Neural Network (CNN) will have a list that contains all our layers. There is no explicit requirement to do it that way, it's simply convinient for this example. Inbetween the layers (which in essence are just Jax arrays, i.e. **leaves** of a PyTree), we have some things which are **not** arrays! These functions, e.g. `relu`, are not things that Jax can differentiate! This means we will have to *filter* those things out. Luckily, Equinox provides several useful filter functions for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad67244a-9093-4fe3-a110-99b03354e63b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, key):\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "        # A simple neural network with 3 hidden layers\n",
    "        self.layers = [\n",
    "            eqx.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=4, key=key1),\n",
    "            eqx.nn.MaxPool2d(kernel_size=2),\n",
    "            jax.nn.relu,\n",
    "            jnp.ravel,\n",
    "            eqx.nn.Linear(1728, 512, key=key2),\n",
    "            jax.nn.sigmoid,\n",
    "            eqx.nn.Linear(512, 64, key=key3),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(64, 10, key=key4),\n",
    "            jax.nn.softmax,\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            # call relu on each layer\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de352850-b04f-41cd-b178-7a0e829f6db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(model: PyTree, x: Array, y: Array):\n",
    "    # Our input has the shape (BATCH_SIZE x 1 x 28 x 28) but our model expects an\n",
    "    # input image with a single input channel;\n",
    "    # the batch dimension was not defined in our model!\n",
    "    # Therefore, we have to use jax.vmap, which - by default - maps over the 0th axis\n",
    "    # (i.e. the batch in our example)\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    # mean squared error loss\n",
    "    return jnp.mean((y - pred_y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c20d9bf-7954-4132-88ac-661239200747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Init our model\n",
    "key, subkey = jax.random.split(jax.random.PRNGKey(32))\n",
    "model = NeuralNetwork(subkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a98b74e-b049-4c71-9ffe-4c123ab96e10",
   "metadata": {},
   "source": [
    "### Our PyTree Model\n",
    "\n",
    "We can print our model to see the PyTree that was generated by Equinox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18958774-ac0f-4eb1-bbb7-69764d7fd87d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  layers=[\n",
      "    Conv2d(\n",
      "      num_spatial_dims=2,\n",
      "      weight=f32[3,1,4,4],\n",
      "      bias=f32[3,1,1],\n",
      "      in_channels=1,\n",
      "      out_channels=3,\n",
      "      kernel_size=(4, 4),\n",
      "      stride=(1, 1),\n",
      "      padding=((0, 0), (0, 0)),\n",
      "      dilation=(1, 1),\n",
      "      groups=1,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    MaxPool2d(\n",
      "      init=-inf,\n",
      "      operation=<function max>,\n",
      "      num_spatial_dims=2,\n",
      "      kernel_size=(2, 2),\n",
      "      stride=(1, 1),\n",
      "      padding=((0, 0), (0, 0)),\n",
      "      use_ceil=False\n",
      "    ),\n",
      "    <wrapped function relu>,\n",
      "    <wrapped function ravel>,\n",
      "    Linear(\n",
      "      weight=f32[512,1728],\n",
      "      bias=f32[512],\n",
      "      in_features=1728,\n",
      "      out_features=512,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    <wrapped function sigmoid>,\n",
      "    Linear(\n",
      "      weight=f32[64,512],\n",
      "      bias=f32[64],\n",
      "      in_features=512,\n",
      "      out_features=64,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    <wrapped function relu>,\n",
      "    Linear(\n",
      "      weight=f32[10,64],\n",
      "      bias=f32[10],\n",
      "      in_features=64,\n",
      "      out_features=10,\n",
      "      use_bias=True\n",
      "    ),\n",
      "    <function softmax>\n",
      "  ]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361fafda-d716-4048-9ac5-ca15a2a1a2a5",
   "metadata": {},
   "source": [
    "If we wanted to perform inference on our model given some data, then we could do it as shown below.\n",
    "\n",
    "(**Note** that you should typically avoid using Jax transformations (such as `vmap`) outside of `jit` as it can be very slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ec7f8db-bb08-434f-a24e-f5ccbdfa5b61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example inference\n",
    "output = jax.vmap(model)(dummy_x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99629b5-e96f-43fa-9e9e-29cbc9da8d4a",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "In the next cells we can see an example of when we should use the filter methods provided by Equinox. For instance, the following code generates an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f719f539-2d5f-460c-8d70-47a721bd414e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument '<function max at 0x7f6d62c443a0>' of type <class 'function'> is not a valid JAX type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This is an error!\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_y\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.virtualenvs/jax-main/lib/python3.8/site-packages/jax/_src/dispatch.py:637\u001b[0m, in \u001b[0;36mcheck_arg\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_arg\u001b[39m(arg):\n\u001b[1;32m    636\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(arg, core\u001b[38;5;241m.\u001b[39mTracer) \u001b[38;5;129;01mor\u001b[39;00m _valid_jaxtype(arg)):\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    638\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAX type.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument '<function max at 0x7f6d62c443a0>' of type <class 'function'> is not a valid JAX type."
     ]
    }
   ],
   "source": [
    "# This is an error!\n",
    "\n",
    "jax.value_and_grad(loss)(model, dummy_x, dummy_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f058f290-d869-43fc-a414-d06295179625",
   "metadata": {},
   "source": [
    "And this makes sense, because when you look back up in the PyTree, you will see some lines that say e.g. `<wrapped function relu>,` . This means there is a function in our PyTree. When Jax tries to compute the gradient, it arrives at this function and is confused (rightfully so!). Therefore, we have to filter out anything that is not a differentiable PyTree leaf! \n",
    "\n",
    "A general rule of thumb is: since `eqx.filter_[...]` is essentially the same as the corresponding Jax functions, whenever you're unsure which one to use, you can simply fall back to the Equinox filter functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1de35485-c4ef-4ec8-aa4e-b7da79c5c4bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09025362\n"
     ]
    }
   ],
   "source": [
    "# This will work!\n",
    "\n",
    "# Getting the initial loss value\n",
    "value, grads = eqx.filter_value_and_grad(loss)(model, dummy_x, dummy_y)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d472b812-e67a-4ee0-b18f-f46a25b00676",
   "metadata": {},
   "source": [
    "As with most machine learning tasks, we need some methods to evaluate our model on some testdata. For this we create the following two functions. \n",
    "\n",
    "Notice, that we used `eqx.filter_jit` instead of `jax.jit` since our PyTree (i.e. our model) contains non-arrays, such as `relu` functions, which can't be interpreted as arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5472245c-537e-49ca-befd-155bbc693cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exchange eqx.filter_jit with jax.jit to see what kind of error we get\n",
    "@eqx.filter_jit\n",
    "def compute_accuracy(model: PyTree, x: Array, y: Array):\n",
    "    \"\"\"\n",
    "    This function takes as input the current model\n",
    "    and computes the average accuracy on a batch.\n",
    "    \"\"\"\n",
    "    preds = jax.vmap(model)(x)\n",
    "    targets = jnp.argmax(y, axis=1)\n",
    "    preds = jnp.argmax(preds, axis=1)\n",
    "    return sum(targets == preds) / len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54927e9e-af16-4480-a502-5b5cadd2d881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model: PyTree, testloader: DataLoader):\n",
    "    \"\"\"\n",
    "    This function evaluates the model on the testing dataset\n",
    "    by computing the average loss and also the average accuracy.\n",
    "    \"\"\"\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    for x, y in testloader:\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        avg_loss += loss(model, x, y)\n",
    "        avg_acc += compute_accuracy(model, x, y)\n",
    "    return avg_loss / len(testloader), avg_acc / len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57f5369a-7f25-42f7-96c0-12447a482acc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(0.09036118, dtype=float32),\n",
       " Array(0.09862659, dtype=float32, weak_type=True))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebdc6b2-a001-41e7-869c-6b151826709c",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now it's time to train our model using Optax! The most important thing to notice here is the `eqx.apply_updates` function! This is a wrapper for the function `optax.apply_updates`. But why do we need it? The reason is that `eqx.filter_value_and_grad` filters our anything that is not differentiable, e.g. functions in our PyTree and instead replaces those fields with `None`. This means that when you compute the gradients and the updates (from `optim.updates(...)`) there will be places with `None`s. The *original* function from Optax would then try to update `None`s using something like `None - learning_rate * None`, which is a big *no no*!\n",
    "\n",
    "Therefore we can use the utility function from Equinox, which has the identical functionality as `optax.apply_updates` only that it skips the `None`s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cc2123c-ad60-4318-8a40-79731f067da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use Optax to optimise the parameters\n",
    "optim = optax.adamw(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cff12310-ce7b-4eb7-8910-84349afe8e93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model: PyTree,\n",
    "    trainloader: DataLoader,\n",
    "    testloader: DataLoader,\n",
    "    optim: GradientTransformation,\n",
    "):\n",
    "    # Get the optimiser state\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def step(model: PyTree, opt_state: PyTree, x: Array, y: Array):\n",
    "        loss_value, grads = eqx.filter_value_and_grad(loss)(model, x, y)\n",
    "        updates, opt_state = optim.update(grads, opt_state, model)\n",
    "        # We use eqx.apply_updates because our gradients contain Nones,\n",
    "        # which Optax can't handle by default\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss_value\n",
    "\n",
    "    for i, (x, y) in enumerate(trainloader):\n",
    "        # PyTorch dataloaders give torch tensors by default,\n",
    "        # so we have to convert them to Numpy arrays first\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        model, opt_state, loss_value = step(model, opt_state, x, y)\n",
    "        # Every 10% of the dataset, print a snapshot of the model's performance\n",
    "        if i % (len(trainloader) // 10) == 0:\n",
    "            eval_loss, acc = evaluate(model, testloader)\n",
    "            current_percentage = np.round((i / len(trainloader)) * 100)\n",
    "            print(f\"step={current_percentage}%, {loss_value=}, {eval_loss=}, {acc=}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bab00c7f-012a-4ed3-bbe2-2a75a2bdd723",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0.0%, loss_value=Array(0.0899663, dtype=float32), eval_loss=Array(0.09051719, dtype=float32), acc=Array(0.10360271, dtype=float32, weak_type=True)\n",
      "step=10.0%, loss_value=Array(0.01999055, dtype=float32), eval_loss=Array(0.0159752, dtype=float32), acc=Array(0.9043591, dtype=float32, weak_type=True)\n",
      "step=20.0%, loss_value=Array(0.0082739, dtype=float32), eval_loss=Array(0.01234425, dtype=float32), acc=Array(0.91988456, dtype=float32, weak_type=True)\n",
      "step=30.0%, loss_value=Array(0.0094656, dtype=float32), eval_loss=Array(0.00933974, dtype=float32), acc=Array(0.9371019, dtype=float32, weak_type=True)\n",
      "step=40.0%, loss_value=Array(0.01203251, dtype=float32), eval_loss=Array(0.00743253, dtype=float32), acc=Array(0.951035, dtype=float32, weak_type=True)\n",
      "step=50.0%, loss_value=Array(0.00362388, dtype=float32), eval_loss=Array(0.00623893, dtype=float32), acc=Array(0.95879775, dtype=float32, weak_type=True)\n",
      "step=59.0%, loss_value=Array(0.00316525, dtype=float32), eval_loss=Array(0.00616679, dtype=float32), acc=Array(0.96098727, dtype=float32, weak_type=True)\n",
      "step=69.0%, loss_value=Array(0.00959226, dtype=float32), eval_loss=Array(0.00640709, dtype=float32), acc=Array(0.9583002, dtype=float32, weak_type=True)\n",
      "step=79.0%, loss_value=Array(0.00147197, dtype=float32), eval_loss=Array(0.00475529, dtype=float32), acc=Array(0.9696457, dtype=float32, weak_type=True)\n",
      "step=89.0%, loss_value=Array(0.00293349, dtype=float32), eval_loss=Array(0.00504184, dtype=float32), acc=Array(0.9648686, dtype=float32, weak_type=True)\n",
      "step=99.0%, loss_value=Array(0.00339005, dtype=float32), eval_loss=Array(0.0046468, dtype=float32), acc=Array(0.96994424, dtype=float32, weak_type=True)\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = fit(model, trainloader, testloader, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a9f2d7-b2d0-427e-8958-d0a381edc333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
