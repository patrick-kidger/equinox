{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f4099f1-d8de-41b2-ad38-5968d56a454b",
   "metadata": {},
   "source": [
    "# U-Net implementation\n",
    "\n",
    "This is an advanced example, providing an implementation of a U-Net architecture.\n",
    "\n",
    "This version is intended for use in a [score-based diffusion](../score_based_diffusion/), so it accepts a `t` argument.\n",
    "\n",
    "This example is available as a Jupyter notebook [here](https://github.com/patrick-kidger/equinox/blob/main/examples/unet.ipynb)\n",
    "\n",
    "Author: Ben Walker ([https://github.com/Benjamin-Walker](https://github.com/Benjamin-Walker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b04c26-bd9c-4130-aa4e-9ab336170df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections.abc import Callable\n",
    "from typing import Optional, Union\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class SinusoidalPosEmb(eqx.Module):\n",
    "    emb: jax.Array\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        half_dim = dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        self.emb = jnp.exp(jnp.arange(half_dim) * -emb)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        emb = x * self.emb\n",
    "        emb = jnp.concatenate((jnp.sin(emb), jnp.cos(emb)), axis=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class LinearTimeSelfAttention(eqx.Module):\n",
    "    group_norm: eqx.nn.GroupNorm\n",
    "    heads: int\n",
    "    to_qkv: eqx.nn.Conv2d\n",
    "    to_out: eqx.nn.Conv2d\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        key,\n",
    "        heads=4,\n",
    "        dim_head=32,\n",
    "    ):\n",
    "        keys = jax.random.split(key, 2)\n",
    "        self.group_norm = eqx.nn.GroupNorm(min(dim // 4, 32), dim)\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = eqx.nn.Conv2d(dim, hidden_dim * 3, 1, key=keys[0])\n",
    "        self.to_out = eqx.nn.Conv2d(hidden_dim, dim, 1, key=keys[1])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        c, h, w = x.shape\n",
    "        x = self.group_norm(x)\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = rearrange(\n",
    "            qkv, \"(qkv heads c) h w -> qkv heads c (h w)\", heads=self.heads, qkv=3\n",
    "        )\n",
    "        k = jax.nn.softmax(k, axis=-1)\n",
    "        context = jnp.einsum(\"hdn,hen->hde\", k, v)\n",
    "        out = jnp.einsum(\"hde,hdn->hen\", context, q)\n",
    "        out = rearrange(\n",
    "            out, \"heads c (h w) -> (heads c) h w\", heads=self.heads, h=h, w=w\n",
    "        )\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "def upsample_2d(y, factor=2):\n",
    "    C, H, W = y.shape\n",
    "    y = jnp.reshape(y, [C, H, 1, W, 1])\n",
    "    y = jnp.tile(y, [1, 1, factor, 1, factor])\n",
    "    return jnp.reshape(y, [C, H * factor, W * factor])\n",
    "\n",
    "\n",
    "def downsample_2d(y, factor=2):\n",
    "    C, H, W = y.shape\n",
    "    y = jnp.reshape(y, [C, H // factor, factor, W // factor, factor])\n",
    "    return jnp.mean(y, axis=[2, 4])\n",
    "\n",
    "\n",
    "def exact_zip(*args):\n",
    "    _len = len(args[0])\n",
    "    for arg in args:\n",
    "        assert len(arg) == _len\n",
    "    return zip(*args)\n",
    "\n",
    "\n",
    "def key_split_allowing_none(key):\n",
    "    if key is None:\n",
    "        return key, None\n",
    "    else:\n",
    "        return jr.split(key)\n",
    "\n",
    "\n",
    "class Residual(eqx.Module):\n",
    "    fn: LinearTimeSelfAttention\n",
    "\n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn\n",
    "\n",
    "    def __call__(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "\n",
    "\n",
    "class ResnetBlock(eqx.Module):\n",
    "    dim_out: int\n",
    "    is_biggan: bool\n",
    "    up: bool\n",
    "    down: bool\n",
    "    dropout_rate: float\n",
    "    time_emb_dim: int\n",
    "    mlp_layers: list[Union[Callable, eqx.nn.Linear]]\n",
    "    scaling: Union[None, Callable, eqx.nn.ConvTranspose2d, eqx.nn.Conv2d]\n",
    "    block1_groupnorm: eqx.nn.GroupNorm\n",
    "    block1_conv: eqx.nn.Conv2d\n",
    "    block2_layers: list[\n",
    "        Union[eqx.nn.GroupNorm, eqx.nn.Dropout, eqx.nn.Conv2d, Callable]\n",
    "    ]\n",
    "    res_conv: eqx.nn.Conv2d\n",
    "    attn: Optional[Residual]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,\n",
    "        dim_out,\n",
    "        is_biggan,\n",
    "        up,\n",
    "        down,\n",
    "        time_emb_dim,\n",
    "        dropout_rate,\n",
    "        is_attn,\n",
    "        heads,\n",
    "        dim_head,\n",
    "        *,\n",
    "        key,\n",
    "    ):\n",
    "        keys = jax.random.split(key, 7)\n",
    "        self.dim_out = dim_out\n",
    "        self.is_biggan = is_biggan\n",
    "        self.up = up\n",
    "        self.down = down\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.time_emb_dim = time_emb_dim\n",
    "\n",
    "        self.mlp_layers = [\n",
    "            jax.nn.silu,\n",
    "            eqx.nn.Linear(time_emb_dim, dim_out, key=keys[0]),\n",
    "        ]\n",
    "        self.block1_groupnorm = eqx.nn.GroupNorm(min(dim_in // 4, 32), dim_in)\n",
    "        self.block1_conv = eqx.nn.Conv2d(dim_in, dim_out, 3, padding=1, key=keys[1])\n",
    "        self.block2_layers = [\n",
    "            eqx.nn.GroupNorm(min(dim_out // 4, 32), dim_out),\n",
    "            jax.nn.silu,\n",
    "            eqx.nn.Dropout(dropout_rate),\n",
    "            eqx.nn.Conv2d(dim_out, dim_out, 3, padding=1, key=keys[2]),\n",
    "        ]\n",
    "\n",
    "        assert not self.up or not self.down\n",
    "\n",
    "        if is_biggan:\n",
    "            if self.up:\n",
    "                self.scaling = upsample_2d\n",
    "            elif self.down:\n",
    "                self.scaling = downsample_2d\n",
    "            else:\n",
    "                self.scaling = None\n",
    "        else:\n",
    "            if self.up:\n",
    "                self.scaling = eqx.nn.ConvTranspose2d(\n",
    "                    dim_in,\n",
    "                    dim_in,\n",
    "                    kernel_size=4,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    key=keys[3],\n",
    "                )\n",
    "            elif self.down:\n",
    "                self.scaling = eqx.nn.Conv2d(\n",
    "                    dim_in,\n",
    "                    dim_in,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    key=keys[4],\n",
    "                )\n",
    "            else:\n",
    "                self.scaling = None\n",
    "        # For DDPM Yang use their own custom layer called NIN, which is\n",
    "        # equivalent to a 1x1 conv\n",
    "        self.res_conv = eqx.nn.Conv2d(dim_in, dim_out, kernel_size=1, key=keys[5])\n",
    "\n",
    "        if is_attn:\n",
    "            self.attn = Residual(\n",
    "                LinearTimeSelfAttention(\n",
    "                    dim_out,\n",
    "                    heads=heads,\n",
    "                    dim_head=dim_head,\n",
    "                    key=keys[6],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            self.attn = None\n",
    "\n",
    "    def __call__(self, x, t, *, key):\n",
    "        C, _, _ = x.shape\n",
    "        # In DDPM, each set of resblocks ends with an up/down sampling. In\n",
    "        # biggan there is a final resblock after the up/downsampling. In this\n",
    "        # code, the biggan approach is taken for both.\n",
    "        # norm -> nonlinearity -> up/downsample -> conv follows Yang\n",
    "        # https://github.dev/yang-song/score_sde/blob/main/models/layerspp.py\n",
    "        h = jax.nn.silu(self.block1_groupnorm(x))\n",
    "        if self.up or self.down:\n",
    "            h = self.scaling(h)  # pyright: ignore\n",
    "            x = self.scaling(x)  # pyright: ignore\n",
    "        h = self.block1_conv(h)\n",
    "\n",
    "        for layer in self.mlp_layers:\n",
    "            t = layer(t)\n",
    "        h = h + t[..., None, None]\n",
    "        for layer in self.block2_layers:\n",
    "            # Precisely 1 dropout layer in block2_layers which requires a key.\n",
    "            if isinstance(layer, eqx.nn.Dropout):\n",
    "                h = layer(h, key=key)\n",
    "            else:\n",
    "                h = layer(h)\n",
    "\n",
    "        if C != self.dim_out or self.up or self.down:\n",
    "            x = self.res_conv(x)\n",
    "\n",
    "        out = (h + x) / jnp.sqrt(2)\n",
    "        if self.attn is not None:\n",
    "            out = self.attn(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNet(eqx.Module):\n",
    "    time_pos_emb: SinusoidalPosEmb\n",
    "    mlp: eqx.nn.MLP\n",
    "    first_conv: eqx.nn.Conv2d\n",
    "    down_res_blocks: list[list[ResnetBlock]]\n",
    "    mid_block1: ResnetBlock\n",
    "    mid_block2: ResnetBlock\n",
    "    ups_res_blocks: list[list[ResnetBlock]]\n",
    "    final_conv_layers: list[Union[Callable, eqx.nn.LayerNorm, eqx.nn.Conv2d]]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_shape: tuple[int, int, int],\n",
    "        is_biggan: bool,\n",
    "        dim_mults: list[int],\n",
    "        hidden_size: int,\n",
    "        heads: int,\n",
    "        dim_head: int,\n",
    "        dropout_rate: float,\n",
    "        num_res_blocks: int,\n",
    "        attn_resolutions: list[int],\n",
    "        *,\n",
    "        key,\n",
    "    ):\n",
    "        keys = jax.random.split(key, 7)\n",
    "        del key\n",
    "\n",
    "        data_channels, in_height, in_width = data_shape\n",
    "\n",
    "        dims = [hidden_size] + [hidden_size * m for m in dim_mults]\n",
    "        in_out = list(exact_zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        self.time_pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            4 * hidden_size,\n",
    "            1,\n",
    "            activation=jax.nn.silu,\n",
    "            key=keys[0],\n",
    "        )\n",
    "        self.first_conv = eqx.nn.Conv2d(\n",
    "            data_channels, hidden_size, kernel_size=3, padding=1, key=keys[1]\n",
    "        )\n",
    "\n",
    "        h, w = in_height, in_width\n",
    "        self.down_res_blocks = []\n",
    "        num_keys = len(in_out) * num_res_blocks - 1\n",
    "        keys_resblock = jr.split(keys[2], num_keys)\n",
    "        i = 0\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            if h in attn_resolutions and w in attn_resolutions:\n",
    "                is_attn = True\n",
    "            else:\n",
    "                is_attn = False\n",
    "            res_blocks = [\n",
    "                ResnetBlock(\n",
    "                    dim_in=dim_in,\n",
    "                    dim_out=dim_out,\n",
    "                    is_biggan=is_biggan,\n",
    "                    up=False,\n",
    "                    down=False,\n",
    "                    time_emb_dim=hidden_size,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    is_attn=is_attn,\n",
    "                    heads=heads,\n",
    "                    dim_head=dim_head,\n",
    "                    key=keys_resblock[i],\n",
    "                )\n",
    "            ]\n",
    "            i += 1\n",
    "            for _ in range(num_res_blocks - 2):\n",
    "                res_blocks.append(\n",
    "                    ResnetBlock(\n",
    "                        dim_in=dim_out,\n",
    "                        dim_out=dim_out,\n",
    "                        is_biggan=is_biggan,\n",
    "                        up=False,\n",
    "                        down=False,\n",
    "                        time_emb_dim=hidden_size,\n",
    "                        dropout_rate=dropout_rate,\n",
    "                        is_attn=is_attn,\n",
    "                        heads=heads,\n",
    "                        dim_head=dim_head,\n",
    "                        key=keys_resblock[i],\n",
    "                    )\n",
    "                )\n",
    "                i += 1\n",
    "            if ind < (len(in_out) - 1):\n",
    "                res_blocks.append(\n",
    "                    ResnetBlock(\n",
    "                        dim_in=dim_out,\n",
    "                        dim_out=dim_out,\n",
    "                        is_biggan=is_biggan,\n",
    "                        up=False,\n",
    "                        down=True,\n",
    "                        time_emb_dim=hidden_size,\n",
    "                        dropout_rate=dropout_rate,\n",
    "                        is_attn=is_attn,\n",
    "                        heads=heads,\n",
    "                        dim_head=dim_head,\n",
    "                        key=keys_resblock[i],\n",
    "                    )\n",
    "                )\n",
    "                i += 1\n",
    "                h, w = h // 2, w // 2\n",
    "            self.down_res_blocks.append(res_blocks)\n",
    "        assert i == num_keys\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = ResnetBlock(\n",
    "            dim_in=mid_dim,\n",
    "            dim_out=mid_dim,\n",
    "            is_biggan=is_biggan,\n",
    "            up=False,\n",
    "            down=False,\n",
    "            time_emb_dim=hidden_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            is_attn=True,\n",
    "            heads=heads,\n",
    "            dim_head=dim_head,\n",
    "            key=keys[3],\n",
    "        )\n",
    "        self.mid_block2 = ResnetBlock(\n",
    "            dim_in=mid_dim,\n",
    "            dim_out=mid_dim,\n",
    "            is_biggan=is_biggan,\n",
    "            up=False,\n",
    "            down=False,\n",
    "            time_emb_dim=hidden_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            is_attn=False,\n",
    "            heads=heads,\n",
    "            dim_head=dim_head,\n",
    "            key=keys[4],\n",
    "        )\n",
    "\n",
    "        self.ups_res_blocks = []\n",
    "        num_keys = len(in_out) * (num_res_blocks + 1) - 1\n",
    "        keys_resblock = jr.split(keys[5], num_keys)\n",
    "        i = 0\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            if h in attn_resolutions and w in attn_resolutions:\n",
    "                is_attn = True\n",
    "            else:\n",
    "                is_attn = False\n",
    "            res_blocks = []\n",
    "            for _ in range(num_res_blocks - 1):\n",
    "                res_blocks.append(\n",
    "                    ResnetBlock(\n",
    "                        dim_in=dim_out * 2,\n",
    "                        dim_out=dim_out,\n",
    "                        is_biggan=is_biggan,\n",
    "                        up=False,\n",
    "                        down=False,\n",
    "                        time_emb_dim=hidden_size,\n",
    "                        dropout_rate=dropout_rate,\n",
    "                        is_attn=is_attn,\n",
    "                        heads=heads,\n",
    "                        dim_head=dim_head,\n",
    "                        key=keys_resblock[i],\n",
    "                    )\n",
    "                )\n",
    "                i += 1\n",
    "            res_blocks.append(\n",
    "                ResnetBlock(\n",
    "                    dim_in=dim_out + dim_in,\n",
    "                    dim_out=dim_in,\n",
    "                    is_biggan=is_biggan,\n",
    "                    up=False,\n",
    "                    down=False,\n",
    "                    time_emb_dim=hidden_size,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    is_attn=is_attn,\n",
    "                    heads=heads,\n",
    "                    dim_head=dim_head,\n",
    "                    key=keys_resblock[i],\n",
    "                )\n",
    "            )\n",
    "            i += 1\n",
    "            if ind < (len(in_out) - 1):\n",
    "                res_blocks.append(\n",
    "                    ResnetBlock(\n",
    "                        dim_in=dim_in,\n",
    "                        dim_out=dim_in,\n",
    "                        is_biggan=is_biggan,\n",
    "                        up=True,\n",
    "                        down=False,\n",
    "                        time_emb_dim=hidden_size,\n",
    "                        dropout_rate=dropout_rate,\n",
    "                        is_attn=is_attn,\n",
    "                        heads=heads,\n",
    "                        dim_head=dim_head,\n",
    "                        key=keys_resblock[i],\n",
    "                    )\n",
    "                )\n",
    "                i += 1\n",
    "                h, w = h * 2, w * 2\n",
    "\n",
    "            self.ups_res_blocks.append(res_blocks)\n",
    "        assert i == num_keys\n",
    "\n",
    "        self.final_conv_layers = [\n",
    "            eqx.nn.GroupNorm(min(hidden_size // 4, 32), hidden_size),\n",
    "            jax.nn.silu,\n",
    "            eqx.nn.Conv2d(hidden_size, data_channels, 1, key=keys[6]),\n",
    "        ]\n",
    "\n",
    "    def __call__(self, t, y, *, key=None):\n",
    "        t = self.time_pos_emb(t)\n",
    "        t = self.mlp(t)\n",
    "        h = self.first_conv(y)\n",
    "        hs = [h]\n",
    "        for res_blocks in self.down_res_blocks:\n",
    "            for res_block in res_blocks:\n",
    "                key, subkey = key_split_allowing_none(key)\n",
    "                h = res_block(h, t, key=subkey)\n",
    "                hs.append(h)\n",
    "\n",
    "        key, subkey = key_split_allowing_none(key)\n",
    "        h = self.mid_block1(h, t, key=subkey)\n",
    "        key, subkey = key_split_allowing_none(key)\n",
    "        h = self.mid_block2(h, t, key=subkey)\n",
    "\n",
    "        for res_blocks in self.ups_res_blocks:\n",
    "            for res_block in res_blocks:\n",
    "                key, subkey = key_split_allowing_none(key)\n",
    "                if res_block.up:\n",
    "                    h = res_block(h, t, key=subkey)\n",
    "                else:\n",
    "                    h = res_block(jnp.concatenate((h, hs.pop()), axis=0), t, key=subkey)\n",
    "\n",
    "        assert len(hs) == 0\n",
    "\n",
    "        for layer in self.final_conv_layers:\n",
    "            h = layer(h)\n",
    "        return h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax38",
   "language": "python",
   "name": "jax38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
