{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x96qLutZxINh"
   },
   "source": [
    "# Vision Transformer (ViT)\n",
    "\n",
    "This example builds a vision transformer model using Equinox, an implementation based on the paper: _An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale_.\n",
    "\n",
    "In addition to this tutorial example, you may also like the ViT implementation [available here in Eqxvision here](https://eqxvision.readthedocs.io/en/latest/api/models/classification/vit/).\n",
    "\n",
    "!!! warning\n",
    "\n",
    "    This example will take a short while to run on a GPU.\n",
    "\n",
    "!!! cite \"Reference\"\n",
    "\n",
    "    [arXiv link](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "    ```bibtex\n",
    "    @inproceedings{dosovitskiy2021an,\n",
    "        title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n",
    "        author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn\n",
    "                and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer\n",
    "                and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},\n",
    "        booktitle={International Conference on Learning Representations},\n",
    "        year={2021},\n",
    "    }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3-NddIhhxINj"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import einops  # https://github.com/arogozhnikov/einops\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import numpy as np\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "\n",
    "# We'll use PyTorch to load the dataset.\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from jaxtyping import Array, Float, PRNGKeyArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bYi-XlXRxINl"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 0.0001\n",
    "dropout_rate = 0.1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "batch_size = 64\n",
    "patch_size = 4\n",
    "num_patches = 64\n",
    "num_steps = 100000\n",
    "image_size = (32, 32, 3)\n",
    "embedding_dim = 512\n",
    "hidden_dim = 256\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "height, width, channels = image_size\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClIEhf1dBa8x"
   },
   "source": [
    "Let's first load the CIFAR10 dataset using torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vcwi4un6CMu_",
    "outputId": "fad94424-789b-46b2-f3df-41f004ddfaf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.Resize((height, width)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((height, width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    \"CIFAR\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_train,\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    \"CIFAR\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_test,\n",
    ")\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-Q4A5H8OQLs"
   },
   "source": [
    "Now Let's start by making the patch embeddings layer that will turn images into embedded patches to be processed then by the attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SFo1GzZvxINl"
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(eqx.Module):\n",
    "    linear: eqx.nn.Embedding\n",
    "    patch_size: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int,\n",
    "        output_shape: int,\n",
    "        patch_size: int,\n",
    "        key: PRNGKeyArray,\n",
    "    ):\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.linear = eqx.nn.Linear(\n",
    "            self.patch_size**2 * input_channels,\n",
    "            output_shape,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, x: Float[Array, \"channels height width\"]\n",
    "    ) -> Float[Array, \"num_patches embedding_dim\"]:\n",
    "        x = einops.rearrange(\n",
    "            x,\n",
    "            \"c (h ph) (w pw) -> (h w) (c ph pw)\",\n",
    "            ph=self.patch_size,\n",
    "            pw=self.patch_size,\n",
    "        )\n",
    "        x = jax.vmap(self.linear)(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOJp3mZNOjJW"
   },
   "source": [
    "After that, we implement the attention block which is the core of the transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mDg-L_9ixINm"
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(eqx.Module):\n",
    "    layer_norm1: eqx.nn.LayerNorm\n",
    "    layer_norm2: eqx.nn.LayerNorm\n",
    "    attention: eqx.nn.MultiheadAttention\n",
    "    linear1: eqx.nn.Linear\n",
    "    linear2: eqx.nn.Linear\n",
    "    dropout1: eqx.nn.Dropout\n",
    "    dropout2: eqx.nn.Dropout\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: int,\n",
    "        hidden_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout_rate: float,\n",
    "        key: PRNGKeyArray,\n",
    "    ):\n",
    "        key1, key2, key3 = jr.split(key, 3)\n",
    "\n",
    "        self.layer_norm1 = eqx.nn.LayerNorm(input_shape)\n",
    "        self.layer_norm2 = eqx.nn.LayerNorm(input_shape)\n",
    "        self.attention = eqx.nn.MultiheadAttention(num_heads, input_shape, key=key1)\n",
    "\n",
    "        self.linear1 = eqx.nn.Linear(input_shape, hidden_dim, key=key2)\n",
    "        self.linear2 = eqx.nn.Linear(hidden_dim, input_shape, key=key3)\n",
    "        self.dropout1 = eqx.nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = eqx.nn.Dropout(dropout_rate)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: Float[Array, \"num_patches embedding_dim\"],\n",
    "        enable_dropout: bool,\n",
    "        key: PRNGKeyArray,\n",
    "    ) -> Float[Array, \"num_patches embedding_dim\"]:\n",
    "        input_x = jax.vmap(self.layer_norm1)(x)\n",
    "        x = x + self.attention(input_x, input_x, input_x)\n",
    "\n",
    "        input_x = jax.vmap(self.layer_norm2)(x)\n",
    "        input_x = jax.vmap(self.linear1)(input_x)\n",
    "        input_x = jax.nn.gelu(input_x)\n",
    "\n",
    "        key1, key2 = jr.split(key, num=2)\n",
    "\n",
    "        input_x = self.dropout1(input_x, inference=not enable_dropout, key=key1)\n",
    "        input_x = jax.vmap(self.linear2)(input_x)\n",
    "        input_x = self.dropout2(input_x, inference=not enable_dropout, key=key2)\n",
    "\n",
    "        x = x + input_x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_RB1ip0PEk4"
   },
   "source": [
    "Lastly, we build the full Vision Transformer model, which is composed of embeddings layers, a series of transformer blocks, and a classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nG6fLPhyQEBx"
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(eqx.Module):\n",
    "    patch_embedding: PatchEmbedding\n",
    "    positional_embedding: jnp.ndarray\n",
    "    cls_token: jnp.ndarray\n",
    "    attention_blocks: list[AttentionBlock]\n",
    "    dropout: eqx.nn.Dropout\n",
    "    mlp: eqx.nn.Sequential\n",
    "    num_layers: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        dropout_rate: float,\n",
    "        patch_size: int,\n",
    "        num_patches: int,\n",
    "        num_classes: int,\n",
    "        key: PRNGKeyArray,\n",
    "    ):\n",
    "        key1, key2, key3, key4, key5 = jr.split(key, 5)\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(channels, embedding_dim, patch_size, key1)\n",
    "\n",
    "        self.positional_embedding = jr.normal(key2, (num_patches + 1, embedding_dim))\n",
    "\n",
    "        self.cls_token = jr.normal(key3, (1, embedding_dim))\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.attention_blocks = [\n",
    "            AttentionBlock(embedding_dim, hidden_dim, num_heads, dropout_rate, key4)\n",
    "            for _ in range(self.num_layers)\n",
    "        ]\n",
    "\n",
    "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.mlp = eqx.nn.Sequential(\n",
    "            [\n",
    "                eqx.nn.LayerNorm(embedding_dim),\n",
    "                eqx.nn.Linear(embedding_dim, num_classes, key=key5),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: Float[Array, \"channels height width\"],\n",
    "        enable_dropout: bool,\n",
    "        key: PRNGKeyArray,\n",
    "    ) -> Float[Array, \"num_classes\"]:\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        x = jnp.concatenate((self.cls_token, x), axis=0)\n",
    "\n",
    "        x += self.positional_embedding[\n",
    "            : x.shape[0]\n",
    "        ]  # Slice to the same length as x, as the positional embedding may be longer.\n",
    "\n",
    "        dropout_key, *attention_keys = jr.split(key, num=self.num_layers + 1)\n",
    "\n",
    "        x = self.dropout(x, inference=not enable_dropout, key=dropout_key)\n",
    "\n",
    "        for block, attention_key in zip(self.attention_blocks, attention_keys):\n",
    "            x = block(x, enable_dropout, key=attention_key)\n",
    "\n",
    "        x = x[0]  # Select the CLS token.\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "agBSRsXVxINn"
   },
   "outputs": [],
   "source": [
    "@eqx.filter_value_and_grad\n",
    "def compute_grads(\n",
    "    model: VisionTransformer, images: jnp.ndarray, labels: jnp.ndarray, key\n",
    "):\n",
    "    logits = jax.vmap(model, in_axes=(0, None, 0))(images, True, key)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "\n",
    "    return jnp.mean(loss)\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def step_model(\n",
    "    model: VisionTransformer,\n",
    "    optimizer: optax.GradientTransformation,\n",
    "    state: optax.OptState,\n",
    "    images: jnp.ndarray,\n",
    "    labels: jnp.ndarray,\n",
    "    key,\n",
    "):\n",
    "    loss, grads = compute_grads(model, images, labels, key)\n",
    "    updates, new_state = optimizer.update(grads, state, model)\n",
    "\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "\n",
    "    return model, new_state, loss\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: VisionTransformer,\n",
    "    optimizer: optax.GradientTransformation,\n",
    "    state: optax.OptState,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    num_steps: int,\n",
    "    print_every: int = 1000,\n",
    "    key=None,\n",
    "):\n",
    "    losses = []\n",
    "\n",
    "    def infinite_trainloader():\n",
    "        while True:\n",
    "            yield from data_loader\n",
    "\n",
    "    for step, batch in zip(range(num_steps), infinite_trainloader()):\n",
    "        images, labels = batch\n",
    "\n",
    "        images = images.numpy()\n",
    "        labels = labels.numpy()\n",
    "\n",
    "        key, *subkeys = jr.split(key, num=batch_size + 1)\n",
    "        subkeys = jnp.array(subkeys)\n",
    "\n",
    "        (model, state, loss) = step_model(\n",
    "            model, optimizer, state, images, labels, subkeys\n",
    "        )\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        if (step % print_every) == 0 or step == num_steps - 1:\n",
    "            print(f\"Step: {step}/{num_steps}, Loss: {loss}.\")\n",
    "\n",
    "    return model, state, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "y3Bm_Xln-rSp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0/100000, Loss: 2.5608019828796387.\n",
      "Step: 1000/100000, Loss: 1.711548089981079.\n",
      "Step: 2000/100000, Loss: 1.4029508829116821.\n",
      "Step: 3000/100000, Loss: 1.405516505241394.\n",
      "Step: 4000/100000, Loss: 1.1661641597747803.\n",
      "Step: 5000/100000, Loss: 1.1351711750030518.\n",
      "Step: 6000/100000, Loss: 1.11599600315094.\n",
      "Step: 7000/100000, Loss: 0.796968936920166.\n",
      "Step: 8000/100000, Loss: 0.6870157718658447.\n",
      "Step: 9000/100000, Loss: 1.0474591255187988.\n",
      "Step: 10000/100000, Loss: 0.9413787722587585.\n",
      "Step: 11000/100000, Loss: 0.8514565229415894.\n",
      "Step: 12000/100000, Loss: 0.6746965646743774.\n",
      "Step: 13000/100000, Loss: 0.7895829677581787.\n",
      "Step: 14000/100000, Loss: 0.6844460964202881.\n",
      "Step: 15000/100000, Loss: 0.6571178436279297.\n",
      "Step: 16000/100000, Loss: 0.5611618757247925.\n",
      "Step: 17000/100000, Loss: 0.610838770866394.\n",
      "Step: 18000/100000, Loss: 0.7180566787719727.\n",
      "Step: 19000/100000, Loss: 0.6528561115264893.\n",
      "Step: 20000/100000, Loss: 0.5517654418945312.\n",
      "Step: 21000/100000, Loss: 0.6301887035369873.\n",
      "Step: 22000/100000, Loss: 0.5667067766189575.\n",
      "Step: 23000/100000, Loss: 0.43517759442329407.\n",
      "Step: 24000/100000, Loss: 0.5348870754241943.\n",
      "Step: 25000/100000, Loss: 0.44732385873794556.\n",
      "Step: 26000/100000, Loss: 0.49118855595588684.\n",
      "Step: 27000/100000, Loss: 0.5242345929145813.\n",
      "Step: 28000/100000, Loss: 0.44588926434516907.\n",
      "Step: 29000/100000, Loss: 0.23619337379932404.\n",
      "Step: 30000/100000, Loss: 0.4560542702674866.\n",
      "Step: 31000/100000, Loss: 0.3148268163204193.\n",
      "Step: 32000/100000, Loss: 0.4813237488269806.\n",
      "Step: 33000/100000, Loss: 0.40532559156417847.\n",
      "Step: 34000/100000, Loss: 0.2517223358154297.\n",
      "Step: 35000/100000, Loss: 0.322698712348938.\n",
      "Step: 36000/100000, Loss: 0.3052283525466919.\n",
      "Step: 37000/100000, Loss: 0.37322986125946045.\n",
      "Step: 38000/100000, Loss: 0.27499520778656006.\n",
      "Step: 39000/100000, Loss: 0.2547920346260071.\n",
      "Step: 40000/100000, Loss: 0.27322614192962646.\n",
      "Step: 41000/100000, Loss: 0.6049947738647461.\n",
      "Step: 42000/100000, Loss: 0.28800976276397705.\n",
      "Step: 43000/100000, Loss: 0.2901820242404938.\n",
      "Step: 44000/100000, Loss: 0.3800655007362366.\n",
      "Step: 45000/100000, Loss: 0.15261484682559967.\n",
      "Step: 46000/100000, Loss: 0.17970965802669525.\n",
      "Step: 47000/100000, Loss: 0.23651015758514404.\n",
      "Step: 48000/100000, Loss: 0.3813527822494507.\n",
      "Step: 49000/100000, Loss: 0.35252541303634644.\n",
      "Step: 50000/100000, Loss: 0.16249465942382812.\n",
      "Step: 51000/100000, Loss: 0.10218428075313568.\n",
      "Step: 52000/100000, Loss: 0.2192973792552948.\n",
      "Step: 53000/100000, Loss: 0.1880446970462799.\n",
      "Step: 54000/100000, Loss: 0.14270251989364624.\n",
      "Step: 55000/100000, Loss: 0.1278090476989746.\n",
      "Step: 56000/100000, Loss: 0.0856819674372673.\n",
      "Step: 57000/100000, Loss: 0.16201086342334747.\n",
      "Step: 58000/100000, Loss: 0.20575015246868134.\n",
      "Step: 59000/100000, Loss: 0.20935538411140442.\n",
      "Step: 60000/100000, Loss: 0.09025183320045471.\n",
      "Step: 61000/100000, Loss: 0.21367806196212769.\n",
      "Step: 62000/100000, Loss: 0.06895419955253601.\n",
      "Step: 63000/100000, Loss: 0.14567255973815918.\n",
      "Step: 64000/100000, Loss: 0.18438486754894257.\n",
      "Step: 65000/100000, Loss: 0.11639232933521271.\n",
      "Step: 66000/100000, Loss: 0.06631053984165192.\n",
      "Step: 67000/100000, Loss: 0.11763929575681686.\n",
      "Step: 68000/100000, Loss: 0.046494871377944946.\n",
      "Step: 69000/100000, Loss: 0.14044761657714844.\n",
      "Step: 70000/100000, Loss: 0.1277393102645874.\n",
      "Step: 71000/100000, Loss: 0.154437854886055.\n",
      "Step: 72000/100000, Loss: 0.15087449550628662.\n",
      "Step: 73000/100000, Loss: 0.05043340474367142.\n",
      "Step: 74000/100000, Loss: 0.3183276355266571.\n",
      "Step: 75000/100000, Loss: 0.15685151517391205.\n",
      "Step: 76000/100000, Loss: 0.13796621561050415.\n",
      "Step: 77000/100000, Loss: 0.1036764532327652.\n",
      "Step: 78000/100000, Loss: 0.08222786337137222.\n",
      "Step: 79000/100000, Loss: 0.1525675356388092.\n",
      "Step: 80000/100000, Loss: 0.06328584253787994.\n",
      "Step: 81000/100000, Loss: 0.1235610619187355.\n",
      "Step: 82000/100000, Loss: 0.03093503788113594.\n",
      "Step: 83000/100000, Loss: 0.07480041682720184.\n",
      "Step: 84000/100000, Loss: 0.016707731410861015.\n",
      "Step: 85000/100000, Loss: 0.0491723008453846.\n",
      "Step: 86000/100000, Loss: 0.0650872215628624.\n",
      "Step: 87000/100000, Loss: 0.08738622069358826.\n",
      "Step: 88000/100000, Loss: 0.10671466588973999.\n",
      "Step: 89000/100000, Loss: 0.11922930181026459.\n",
      "Step: 90000/100000, Loss: 0.1234014481306076.\n",
      "Step: 91000/100000, Loss: 0.08588997274637222.\n",
      "Step: 92000/100000, Loss: 0.036773063242435455.\n",
      "Step: 93000/100000, Loss: 0.03425668179988861.\n",
      "Step: 94000/100000, Loss: 0.21202465891838074.\n",
      "Step: 95000/100000, Loss: 0.26020047068595886.\n",
      "Step: 96000/100000, Loss: 0.154791459441185.\n",
      "Step: 97000/100000, Loss: 0.1340092271566391.\n",
      "Step: 98000/100000, Loss: 0.11398129910230637.\n",
      "Step: 99000/100000, Loss: 0.16246598958969116.\n",
      "Step: 99999/100000, Loss: 0.04668630287051201.\n"
     ]
    }
   ],
   "source": [
    "key = jr.PRNGKey(2003)\n",
    "\n",
    "model = VisionTransformer(\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    dropout_rate=dropout_rate,\n",
    "    patch_size=patch_size,\n",
    "    num_patches=num_patches,\n",
    "    num_classes=num_classes,\n",
    "    key=key,\n",
    ")\n",
    "\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=lr,\n",
    "    b1=beta1,\n",
    "    b2=beta2,\n",
    ")\n",
    "\n",
    "state = optimizer.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "\n",
    "model, state, losses = train(model, optimizer, state, trainloader, num_steps, key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4GPbpuMQEB1"
   },
   "source": [
    "And now let's see how the vision transformer performs on the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZu5pMRZW3tF",
    "outputId": "d74d06b7-0340-4e4f-b723-8e5d532aecb5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.13661858974359%\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "\n",
    "for batch in range(len(test_dataset) // batch_size):\n",
    "    images, labels = next(iter(testloader))\n",
    "\n",
    "    logits = jax.vmap(functools.partial(model, enable_dropout=False))(\n",
    "        images.numpy(), key=jax.random.split(key, num=batch_size)\n",
    "    )\n",
    "\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy = jnp.mean(predictions == labels.numpy())\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "print(f\"Accuracy: {np.sum(accuracies) / len(accuracies) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6i8KZl4Ba87"
   },
   "source": [
    "Of course this is not the best accuracy you can get on CIFAR10, but with more training and hyperparameter tuning, you can get better results using the vision transformer."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
