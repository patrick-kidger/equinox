{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feef57f0-b351-46af-821d-cf43bcbaf60d",
   "metadata": {},
   "source": [
    "# Stateful operations (e.g. BatchNorm)\n",
    "\n",
    "Some layers, such as [`equinox.nn.BatchNorm`][] are sometimes called \"stateful\": this refers to the fact that they take an additional input (in the case of BatchNorm, the running statistics) and return an additional output (the updated running statistics).\n",
    "\n",
    "This just means that we need to plumb an extra input and output through our models. This example demonstrates both [`equinox.nn.BatchNorm`][] and [`equinox.nn.SpectralNorm`][].\n",
    "\n",
    "This example is available as a Jupyter notebook [here](https://github.com/patrick-kidger/equinox/blob/main/examples/stateful.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e60412-cf59-4ba6-bbdb-3117e914f949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import optax  # https://github.com/deepmind/optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1c7d0d5-26bf-405b-a208-6cda4c12f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is just a weird mish-mash of layers for demonstration purposes, it isn't\n",
    "# doing any clever.\n",
    "class Model(eqx.Module):\n",
    "    norm1: eqx.nn.BatchNorm\n",
    "    spectral_linear: eqx.nn.SpectralNorm[eqx.nn.Linear]\n",
    "    norm2: eqx.nn.BatchNorm\n",
    "    linear1: eqx.nn.Linear\n",
    "    linear2: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, key):\n",
    "        key1, key2, key3, key4 = jr.split(key, 4)\n",
    "        self.norm1 = eqx.nn.BatchNorm(input_size=3, axis_name=\"batch\")\n",
    "        self.spectral_linear = eqx.nn.SpectralNorm(\n",
    "            layer=eqx.nn.Linear(in_features=3, out_features=32, key=key1),\n",
    "            weight_name=\"weight\",\n",
    "            key=key2,\n",
    "        )\n",
    "        self.norm2 = eqx.nn.BatchNorm(input_size=32, axis_name=\"batch\")\n",
    "        self.linear1 = eqx.nn.Linear(in_features=32, out_features=32, key=key3)\n",
    "        self.linear2 = eqx.nn.Linear(in_features=32, out_features=3, key=key4)\n",
    "\n",
    "    def __call__(self, x, state):\n",
    "        x, state = self.norm1(x, state)\n",
    "        x, state = self.spectral_linear(x, state)\n",
    "        x = jax.nn.relu(x)\n",
    "        x, state = self.norm2(x, state)\n",
    "        x = self.linear1(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8300043-aa88-4676-8501-ff338f1e741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, state, xs, ys):\n",
    "    # The `axis_name` argument is needed specifically for `BatchNorm`: so it knows\n",
    "    #     what axis to compute batch statistics over.\n",
    "    # The `in_axes` and `out_axes` are needed so that `state` isn't batched.\n",
    "    batch_model = jax.vmap(\n",
    "        model, axis_name=\"batch\", in_axes=(0, None), out_axes=(0, None)\n",
    "    )\n",
    "    pred_ys, state = batch_model(xs, state)\n",
    "    loss = jnp.mean((pred_ys - ys) ** 2)\n",
    "    return loss, state\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(model, state, opt_state, xs, ys):\n",
    "    grads, state = eqx.filter_grad(compute_loss, has_aux=True)(model, state, xs, ys)\n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, state, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a7f49fd-9b91-462a-b1d5-dc694f56411e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_size = 10\n",
    "learning_rate = 3e-4\n",
    "steps = 5\n",
    "seed = 5678\n",
    "\n",
    "key = jr.PRNGKey(seed)\n",
    "mkey, xkey, xkey2 = jr.split(key, 3)\n",
    "model = Model(mkey)\n",
    "state = eqx.nn.State(model)\n",
    "xs = jr.normal(xkey, (dataset_size, 3))\n",
    "ys = jnp.sin(xs) + 1\n",
    "optim = optax.adam(learning_rate)\n",
    "opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "\n",
    "# Full-batch gradient descent in this simple example.\n",
    "for _ in range(steps):\n",
    "    model, state, opt_state = make_step(model, state, opt_state, xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2135ba-07dc-4ce7-89c1-105bd699e78d",
   "metadata": {},
   "source": [
    "Overall, we see that this should be relatively straightforward!\n",
    "\n",
    "When calling `state = eqx.nn.State(model)`, then the model PyTree is iterated over, and any stateful layers store their initial states in the resulting `state` object. The `state` object is itself also a PyTree, so it can just be passed around in the usual way.\n",
    "\n",
    "In this example, `state` will store the running statistics for `BatchNorm`, and U-V power iterations for `SpectralNorm`.\n",
    "\n",
    "Subsequently, we just need to thread the `state` object in-and-out of every call. Each time a new state object is returned. (And the old state object should not be reused.)\n",
    "\n",
    "Finally, let's use our trained model to perform inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c53cffd0-afd2-49c3-aa21-79eca5792598",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = eqx.nn.inference_mode(model)\n",
    "inference_model = eqx.Partial(inference_model, state=state)\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def evaluate(model, xs):\n",
    "    # discard state\n",
    "    out, _ = jax.vmap(model)(xs)\n",
    "    return out\n",
    "\n",
    "\n",
    "test_dataset_size = 5\n",
    "test_xs = jr.normal(xkey2, (test_dataset_size, 3))\n",
    "pred_test_ys = evaluate(inference_model, test_xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e4395-9006-465e-ba0b-a4b0d8131bd7",
   "metadata": {},
   "source": [
    "Here, we don't need the updated state object that is output, so we just discard it.\n",
    "\n",
    "(Also, don't forget to set the `inference` flags.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax38",
   "language": "python",
   "name": "jax38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
