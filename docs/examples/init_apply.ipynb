{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14148da3-23fb-480b-ae31-93878cda86fa",
   "metadata": {},
   "source": [
    "# Compatibility with init-apply libraries\n",
    "\n",
    "Existing JAX neural network libraries have sometimes followed the \"init/apply\" approach, in which the parameters of a network are initialised with a function `init()`, and then the forward pass through a model is specified with `apply()`. For example [Stax](https://jax.readthedocs.io/en/latest/jax.example_libraries.stax.html) follows this approach.\n",
    "\n",
    "As a result, some third-party libraries assume that your model is specified by an `init()` and an `apply()` function, and that the parameters returned from `init()` are all JIT-trace-able and grad-able.\n",
    "\n",
    "Equinox can be made to fit with this style very easily, like so.\n",
    "\n",
    "This example is available as a Jupyter notebook [here](https://github.com/patrick-kidger/equinox/blob/main/docs/examples/init_apply.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05bcc516-c80a-492b-af63-0e5dec66f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "\n",
    "\n",
    "def make_mlp(in_size, out_size, width_size, depth, *, key):\n",
    "    mlp = eqx.nn.MLP(\n",
    "        in_size, out_size, width_size, depth, key=key\n",
    "    )  # insert your model here\n",
    "    params, static = eqx.partition(mlp, eqx.is_inexact_array)\n",
    "\n",
    "    def init_fn():\n",
    "        return params\n",
    "\n",
    "    def apply_fn(_params, x):\n",
    "        model = eqx.combine(_params, static)\n",
    "        return model(x)\n",
    "\n",
    "    return init_fn, apply_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5965f2e9-483a-475e-b316-a595daf6eb0f",
   "metadata": {},
   "source": [
    "And that's all there is to it.\n",
    "\n",
    "Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e25e63c7-3cb6-4727-9c28-0016b03acb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import jax.tree_util as jtu\n",
    "\n",
    "\n",
    "def main(in_size=2, seed=5678):\n",
    "    key = jrandom.PRNGKey(seed)\n",
    "\n",
    "    init_fn, apply_fn = make_mlp(\n",
    "        in_size=in_size, out_size=1, width_size=8, depth=1, key=key\n",
    "    )\n",
    "\n",
    "    x = jnp.arange(in_size)  # sample data\n",
    "    params = init_fn()\n",
    "    y1 = apply_fn(params, x)\n",
    "    params = jtu.tree_map(lambda p: p + 1, params)  # \"stochastic gradient descent\"\n",
    "    y2 = apply_fn(params, x)\n",
    "    assert y1 != y2\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax0226",
   "language": "python",
   "name": "jax0226"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
